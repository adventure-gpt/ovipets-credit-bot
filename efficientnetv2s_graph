digraph {
	graph [size="435.9,435.9"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1675296937920 [label="
 ()" fillcolor=darkolivegreen1]
	1675297135536 [label=SumBackward0]
	1675297135440 -> 1675297135536
	1675297135440 [label=AddmmBackward0]
	1675297135968 -> 1675297135440
	1675275205664 [label="classifier.bias
 (10)" fillcolor=lightblue]
	1675275205664 -> 1675297135968
	1675297135968 [label=AccumulateGrad]
	1675297135344 -> 1675297135440
	1675297135344 [label=ViewBackward0]
	1675297136112 -> 1675297135344
	1675297136112 [label=MeanBackward1]
	1675297135008 -> 1675297136112
	1675297135008 [label=SiluBackward0]
	1675297134960 -> 1675297135008
	1675297134960 [label=NativeBatchNormBackward0]
	1675297134768 -> 1675297134960
	1675297134768 [label=ConvolutionBackward0]
	1675297134384 -> 1675297134768
	1675297134384 [label=AddBackward0]
	1675297134192 -> 1675297134384
	1675297134192 [label=NativeBatchNormBackward0]
	1675297134000 -> 1675297134192
	1675297134000 [label=ConvolutionBackward0]
	1675297133616 -> 1675297134000
	1675297133616 [label=MulBackward0]
	1675297133328 -> 1675297133616
	1675297133328 [label=SiluBackward0]
	1675297133136 -> 1675297133328
	1675297133136 [label=NativeBatchNormBackward0]
	1675297132896 -> 1675297133136
	1675297132896 [label=ConvolutionBackward0]
	1675297132752 -> 1675297132896
	1675297132752 [label=SiluBackward0]
	1675297132656 -> 1675297132752
	1675297132656 [label=NativeBatchNormBackward0]
	1675297107728 -> 1675297132656
	1675297107728 [label=ConvolutionBackward0]
	1675297134144 -> 1675297107728
	1675297134144 [label=AddBackward0]
	1675297107152 -> 1675297134144
	1675297107152 [label=NativeBatchNormBackward0]
	1675297106912 -> 1675297107152
	1675297106912 [label=ConvolutionBackward0]
	1675297106672 -> 1675297106912
	1675297106672 [label=MulBackward0]
	1675297106384 -> 1675297106672
	1675297106384 [label=SiluBackward0]
	1675297106048 -> 1675297106384
	1675297106048 [label=NativeBatchNormBackward0]
	1675297105952 -> 1675297106048
	1675297105952 [label=ConvolutionBackward0]
	1675297105712 -> 1675297105952
	1675297105712 [label=SiluBackward0]
	1675297105424 -> 1675297105712
	1675297105424 [label=NativeBatchNormBackward0]
	1675297105232 -> 1675297105424
	1675297105232 [label=ConvolutionBackward0]
	1675297107248 -> 1675297105232
	1675297107248 [label=AddBackward0]
	1675297104848 -> 1675297107248
	1675297104848 [label=NativeBatchNormBackward0]
	1675297104512 -> 1675297104848
	1675297104512 [label=ConvolutionBackward0]
	1675297104368 -> 1675297104512
	1675297104368 [label=MulBackward0]
	1675297104080 -> 1675297104368
	1675297104080 [label=SiluBackward0]
	1675297103984 -> 1675297104080
	1675297103984 [label=NativeBatchNormBackward0]
	1675297074816 -> 1675297103984
	1675297074816 [label=ConvolutionBackward0]
	1675297074672 -> 1675297074816
	1675297074672 [label=SiluBackward0]
	1675297074384 -> 1675297074672
	1675297074384 [label=NativeBatchNormBackward0]
	1675297074192 -> 1675297074384
	1675297074192 [label=ConvolutionBackward0]
	1675297104800 -> 1675297074192
	1675297104800 [label=AddBackward0]
	1675297073808 -> 1675297104800
	1675297073808 [label=NativeBatchNormBackward0]
	1675297073520 -> 1675297073808
	1675297073520 [label=ConvolutionBackward0]
	1675297073136 -> 1675297073520
	1675297073136 [label=MulBackward0]
	1675297072944 -> 1675297073136
	1675297072944 [label=SiluBackward0]
	1675297072608 -> 1675297072944
	1675297072608 [label=NativeBatchNormBackward0]
	1675297072560 -> 1675297072608
	1675297072560 [label=ConvolutionBackward0]
	1675297072224 -> 1675297072560
	1675297072224 [label=SiluBackward0]
	1675297072080 -> 1675297072224
	1675297072080 [label=NativeBatchNormBackward0]
	1675297071888 -> 1675297072080
	1675297071888 [label=ConvolutionBackward0]
	1675297073760 -> 1675297071888
	1675297073760 [label=AddBackward0]
	1675297071408 -> 1675297073760
	1675297071408 [label=NativeBatchNormBackward0]
	1675297071216 -> 1675297071408
	1675297071216 [label=ConvolutionBackward0]
	1675297050288 -> 1675297071216
	1675297050288 [label=MulBackward0]
	1675297050000 -> 1675297050288
	1675297050000 [label=SiluBackward0]
	1675297049760 -> 1675297050000
	1675297049760 [label=NativeBatchNormBackward0]
	1675297049712 -> 1675297049760
	1675297049712 [label=ConvolutionBackward0]
	1675297049328 -> 1675297049712
	1675297049328 [label=SiluBackward0]
	1675297049040 -> 1675297049328
	1675297049040 [label=NativeBatchNormBackward0]
	1675297048944 -> 1675297049040
	1675297048944 [label=ConvolutionBackward0]
	1675297071360 -> 1675297048944
	1675297071360 [label=AddBackward0]
	1675297048464 -> 1675297071360
	1675297048464 [label=NativeBatchNormBackward0]
	1675297048224 -> 1675297048464
	1675297048224 [label=ConvolutionBackward0]
	1675297047984 -> 1675297048224
	1675297047984 [label=MulBackward0]
	1675297047696 -> 1675297047984
	1675297047696 [label=SiluBackward0]
	1675297047360 -> 1675297047696
	1675297047360 [label=NativeBatchNormBackward0]
	1675297047264 -> 1675297047360
	1675297047264 [label=ConvolutionBackward0]
	1675297047024 -> 1675297047264
	1675297047024 [label=SiluBackward0]
	1675297046736 -> 1675297047024
	1675297046736 [label=NativeBatchNormBackward0]
	1675297046640 -> 1675297046736
	1675297046640 [label=ConvolutionBackward0]
	1675297048560 -> 1675297046640
	1675297048560 [label=AddBackward0]
	1675297017424 -> 1675297048560
	1675297017424 [label=NativeBatchNormBackward0]
	1675297017136 -> 1675297017424
	1675297017136 [label=ConvolutionBackward0]
	1675297016704 -> 1675297017136
	1675297016704 [label=MulBackward0]
	1675297016560 -> 1675297016704
	1675297016560 [label=SiluBackward0]
	1675297016368 -> 1675297016560
	1675297016368 [label=NativeBatchNormBackward0]
	1675297016128 -> 1675297016368
	1675297016128 [label=ConvolutionBackward0]
	1675297015984 -> 1675297016128
	1675297015984 [label=SiluBackward0]
	1675297015696 -> 1675297015984
	1675297015696 [label=NativeBatchNormBackward0]
	1675297015504 -> 1675297015696
	1675297015504 [label=ConvolutionBackward0]
	1675297017520 -> 1675297015504
	1675297017520 [label=AddBackward0]
	1675297015120 -> 1675297017520
	1675297015120 [label=NativeBatchNormBackward0]
	1675297014832 -> 1675297015120
	1675297014832 [label=ConvolutionBackward0]
	1675297014448 -> 1675297014832
	1675297014448 [label=MulBackward0]
	1675297014256 -> 1675297014448
	1675297014256 [label=SiluBackward0]
	1675297014064 -> 1675297014256
	1675297014064 [label=NativeBatchNormBackward0]
	1675297013872 -> 1675297014064
	1675297013872 [label=ConvolutionBackward0]
	1675296984752 -> 1675297013872
	1675296984752 [label=SiluBackward0]
	1675296984560 -> 1675296984752
	1675296984560 [label=NativeBatchNormBackward0]
	1675296984368 -> 1675296984560
	1675296984368 [label=ConvolutionBackward0]
	1675297015072 -> 1675296984368
	1675297015072 [label=AddBackward0]
	1675296983984 -> 1675297015072
	1675296983984 [label=NativeBatchNormBackward0]
	1675296983792 -> 1675296983984
	1675296983792 [label=ConvolutionBackward0]
	1675296983408 -> 1675296983792
	1675296983408 [label=MulBackward0]
	1675296983120 -> 1675296983408
	1675296983120 [label=SiluBackward0]
	1675296982880 -> 1675296983120
	1675296982880 [label=NativeBatchNormBackward0]
	1675296982832 -> 1675296982880
	1675296982832 [label=ConvolutionBackward0]
	1675296982448 -> 1675296982832
	1675296982448 [label=SiluBackward0]
	1675296982160 -> 1675296982448
	1675296982160 [label=NativeBatchNormBackward0]
	1675296982064 -> 1675296982160
	1675296982064 [label=ConvolutionBackward0]
	1675296983936 -> 1675296982064
	1675296983936 [label=AddBackward0]
	1675296981584 -> 1675296983936
	1675296981584 [label=NativeBatchNormBackward0]
	1675296981296 -> 1675296981584
	1675296981296 [label=ConvolutionBackward0]
	1675296981104 -> 1675296981296
	1675296981104 [label=MulBackward0]
	1675296952080 -> 1675296981104
	1675296952080 [label=SiluBackward0]
	1675296951744 -> 1675296952080
	1675296951744 [label=NativeBatchNormBackward0]
	1675296951648 -> 1675296951744
	1675296951648 [label=ConvolutionBackward0]
	1675296951408 -> 1675296951648
	1675296951408 [label=SiluBackward0]
	1675296951120 -> 1675296951408
	1675296951120 [label=NativeBatchNormBackward0]
	1675296950928 -> 1675296951120
	1675296950928 [label=ConvolutionBackward0]
	1675296981680 -> 1675296950928
	1675296981680 [label=AddBackward0]
	1675296950544 -> 1675296981680
	1675296950544 [label=NativeBatchNormBackward0]
	1675296950256 -> 1675296950544
	1675296950256 [label=ConvolutionBackward0]
	1675296949824 -> 1675296950256
	1675296949824 [label=MulBackward0]
	1675296949776 -> 1675296949824
	1675296949776 [label=SiluBackward0]
	1675296949488 -> 1675296949776
	1675296949488 [label=NativeBatchNormBackward0]
	1675296949296 -> 1675296949488
	1675296949296 [label=ConvolutionBackward0]
	1675296948864 -> 1675296949296
	1675296948864 [label=SiluBackward0]
	1675296948720 -> 1675296948864
	1675296948720 [label=NativeBatchNormBackward0]
	1675296948624 -> 1675296948720
	1675296948624 [label=ConvolutionBackward0]
	1675296950640 -> 1675296948624
	1675296950640 [label=AddBackward0]
	1675296948336 -> 1675296950640
	1675296948336 [label=NativeBatchNormBackward0]
	1675296927408 -> 1675296948336
	1675296927408 [label=ConvolutionBackward0]
	1675296927024 -> 1675296927408
	1675296927024 [label=MulBackward0]
	1675296926832 -> 1675296927024
	1675296926832 [label=SiluBackward0]
	1675296926640 -> 1675296926832
	1675296926640 [label=NativeBatchNormBackward0]
	1675296926448 -> 1675296926640
	1675296926448 [label=ConvolutionBackward0]
	1675296926064 -> 1675296926448
	1675296926064 [label=SiluBackward0]
	1675296925872 -> 1675296926064
	1675296925872 [label=NativeBatchNormBackward0]
	1675296925776 -> 1675296925872
	1675296925776 [label=ConvolutionBackward0]
	1675296948384 -> 1675296925776
	1675296948384 [label=AddBackward0]
	1675296925200 -> 1675296948384
	1675296925200 [label=NativeBatchNormBackward0]
	1675296924864 -> 1675296925200
	1675296924864 [label=ConvolutionBackward0]
	1675296924576 -> 1675296924864
	1675296924576 [label=MulBackward0]
	1675296924432 -> 1675296924576
	1675296924432 [label=SiluBackward0]
	1675296924192 -> 1675296924432
	1675296924192 [label=NativeBatchNormBackward0]
	1675296924144 -> 1675296924192
	1675296924144 [label=ConvolutionBackward0]
	1675296923760 -> 1675296924144
	1675296923760 [label=SiluBackward0]
	1675296894736 -> 1675296923760
	1675296894736 [label=NativeBatchNormBackward0]
	1675296894640 -> 1675296894736
	1675296894640 [label=ConvolutionBackward0]
	1675296925296 -> 1675296894640
	1675296925296 [label=AddBackward0]
	1675296894160 -> 1675296925296
	1675296894160 [label=NativeBatchNormBackward0]
	1675296893872 -> 1675296894160
	1675296893872 [label=ConvolutionBackward0]
	1675296893536 -> 1675296893872
	1675296893536 [label=MulBackward0]
	1675296893392 -> 1675296893536
	1675296893392 [label=SiluBackward0]
	1675296893104 -> 1675296893392
	1675296893104 [label=NativeBatchNormBackward0]
	1675296892912 -> 1675296893104
	1675296892912 [label=ConvolutionBackward0]
	1675296892624 -> 1675296892912
	1675296892624 [label=SiluBackward0]
	1675296892432 -> 1675296892624
	1675296892432 [label=NativeBatchNormBackward0]
	1675296892240 -> 1675296892432
	1675296892240 [label=ConvolutionBackward0]
	1675296894256 -> 1675296892240
	1675296894256 [label=NativeBatchNormBackward0]
	1675296891856 -> 1675296894256
	1675296891856 [label=ConvolutionBackward0]
	1675296891472 -> 1675296891856
	1675296891472 [label=MulBackward0]
	1675296891136 -> 1675296891472
	1675296891136 [label=SiluBackward0]
	1675296891088 -> 1675296891136
	1675296891088 [label=NativeBatchNormBackward0]
	1675296890992 -> 1675296891088
	1675296890992 [label=ConvolutionBackward0]
	1675296861776 -> 1675296890992
	1675296861776 [label=ConstantPadNdBackward0]
	1675296861440 -> 1675296861776
	1675296861440 [label=SiluBackward0]
	1675296861344 -> 1675296861440
	1675296861344 [label=NativeBatchNormBackward0]
	1675296861296 -> 1675296861344
	1675296861296 [label=ConvolutionBackward0]
	1675296860912 -> 1675296861296
	1675296860912 [label=AddBackward0]
	1675296860624 -> 1675296860912
	1675296860624 [label=NativeBatchNormBackward0]
	1675296860432 -> 1675296860624
	1675296860432 [label=ConvolutionBackward0]
	1675296860144 -> 1675296860432
	1675296860144 [label=MulBackward0]
	1675296859952 -> 1675296860144
	1675296859952 [label=SiluBackward0]
	1675296859760 -> 1675296859952
	1675296859760 [label=NativeBatchNormBackward0]
	1675296859568 -> 1675296859760
	1675296859568 [label=ConvolutionBackward0]
	1675296859184 -> 1675296859568
	1675296859184 [label=SiluBackward0]
	1675296858992 -> 1675296859184
	1675296858992 [label=NativeBatchNormBackward0]
	1675296858896 -> 1675296858992
	1675296858896 [label=ConvolutionBackward0]
	1675296860720 -> 1675296858896
	1675296860720 [label=AddBackward0]
	1675296858320 -> 1675296860720
	1675296858320 [label=NativeBatchNormBackward0]
	1675296858224 -> 1675296858320
	1675296858224 [label=ConvolutionBackward0]
	1675296837296 -> 1675296858224
	1675296837296 [label=MulBackward0]
	1675296837008 -> 1675296837296
	1675296837008 [label=SiluBackward0]
	1675296836720 -> 1675296837008
	1675296836720 [label=NativeBatchNormBackward0]
	1675296836480 -> 1675296836720
	1675296836480 [label=ConvolutionBackward0]
	1675296836192 -> 1675296836480
	1675296836192 [label=SiluBackward0]
	1675296836048 -> 1675296836192
	1675296836048 [label=NativeBatchNormBackward0]
	1675296835952 -> 1675296836048
	1675296835952 [label=ConvolutionBackward0]
	1675296858416 -> 1675296835952
	1675296858416 [label=AddBackward0]
	1675296835472 -> 1675296858416
	1675296835472 [label=NativeBatchNormBackward0]
	1675296835184 -> 1675296835472
	1675296835184 [label=ConvolutionBackward0]
	1675296834848 -> 1675296835184
	1675296834848 [label=MulBackward0]
	1675296834704 -> 1675296834848
	1675296834704 [label=SiluBackward0]
	1675296834416 -> 1675296834704
	1675296834416 [label=NativeBatchNormBackward0]
	1675296834224 -> 1675296834416
	1675296834224 [label=ConvolutionBackward0]
	1675296833888 -> 1675296834224
	1675296833888 [label=SiluBackward0]
	1675296833744 -> 1675296833888
	1675296833744 [label=NativeBatchNormBackward0]
	1675296833648 -> 1675296833744
	1675296833648 [label=ConvolutionBackward0]
	1675296835568 -> 1675296833648
	1675296835568 [label=AddBackward0]
	1675296804336 -> 1675296835568
	1675296804336 [label=NativeBatchNormBackward0]
	1675296804000 -> 1675296804336
	1675296804000 [label=ConvolutionBackward0]
	1675296803712 -> 1675296804000
	1675296803712 [label=MulBackward0]
	1675296803664 -> 1675296803712
	1675296803664 [label=SiluBackward0]
	1675296803376 -> 1675296803664
	1675296803376 [label=NativeBatchNormBackward0]
	1675296803184 -> 1675296803376
	1675296803184 [label=ConvolutionBackward0]
	1675296802752 -> 1675296803184
	1675296802752 [label=SiluBackward0]
	1675296802704 -> 1675296802752
	1675296802704 [label=NativeBatchNormBackward0]
	1675296802512 -> 1675296802704
	1675296802512 [label=ConvolutionBackward0]
	1675296804288 -> 1675296802512
	1675296804288 [label=AddBackward0]
	1675296801936 -> 1675296804288
	1675296801936 [label=NativeBatchNormBackward0]
	1675296801696 -> 1675296801936
	1675296801696 [label=ConvolutionBackward0]
	1675296801456 -> 1675296801696
	1675296801456 [label=MulBackward0]
	1675296801168 -> 1675296801456
	1675296801168 [label=SiluBackward0]
	1675296800976 -> 1675296801168
	1675296800976 [label=NativeBatchNormBackward0]
	1675296800880 -> 1675296800976
	1675296800880 [label=ConvolutionBackward0]
	1675296767664 -> 1675296800880
	1675296767664 [label=SiluBackward0]
	1675296767472 -> 1675296767664
	1675296767472 [label=NativeBatchNormBackward0]
	1675296767376 -> 1675296767472
	1675296767376 [label=ConvolutionBackward0]
	1675296802032 -> 1675296767376
	1675296802032 [label=AddBackward0]
	1675296766800 -> 1675296802032
	1675296766800 [label=NativeBatchNormBackward0]
	1675296766464 -> 1675296766800
	1675296766464 [label=ConvolutionBackward0]
	1675296766320 -> 1675296766464
	1675296766320 [label=MulBackward0]
	1675296766032 -> 1675296766320
	1675296766032 [label=SiluBackward0]
	1675296765744 -> 1675296766032
	1675296765744 [label=NativeBatchNormBackward0]
	1675296765504 -> 1675296765744
	1675296765504 [label=ConvolutionBackward0]
	1675296765360 -> 1675296765504
	1675296765360 [label=SiluBackward0]
	1675296765072 -> 1675296765360
	1675296765072 [label=NativeBatchNormBackward0]
	1675296764880 -> 1675296765072
	1675296764880 [label=ConvolutionBackward0]
	1675296766896 -> 1675296764880
	1675296766896 [label=AddBackward0]
	1675296764400 -> 1675296766896
	1675296764400 [label=NativeBatchNormBackward0]
	1675296764208 -> 1675296764400
	1675296764208 [label=ConvolutionBackward0]
	1675296764016 -> 1675296764208
	1675296764016 [label=MulBackward0]
	1675296743184 -> 1675296764016
	1675296743184 [label=SiluBackward0]
	1675296742896 -> 1675296743184
	1675296742896 [label=NativeBatchNormBackward0]
	1675296742704 -> 1675296742896
	1675296742704 [label=ConvolutionBackward0]
	1675296742368 -> 1675296742704
	1675296742368 [label=SiluBackward0]
	1675296742224 -> 1675296742368
	1675296742224 [label=NativeBatchNormBackward0]
	1675296742032 -> 1675296742224
	1675296742032 [label=ConvolutionBackward0]
	1675296764496 -> 1675296742032
	1675296764496 [label=AddBackward0]
	1675296741552 -> 1675296764496
	1675296741552 [label=NativeBatchNormBackward0]
	1675296741360 -> 1675296741552
	1675296741360 [label=ConvolutionBackward0]
	1675296740976 -> 1675296741360
	1675296740976 [label=MulBackward0]
	1675296740688 -> 1675296740976
	1675296740688 [label=SiluBackward0]
	1675296740496 -> 1675296740688
	1675296740496 [label=NativeBatchNormBackward0]
	1675296740256 -> 1675296740496
	1675296740256 [label=ConvolutionBackward0]
	1675296739968 -> 1675296740256
	1675296739968 [label=SiluBackward0]
	1675296739920 -> 1675296739968
	1675296739920 [label=NativeBatchNormBackward0]
	1675296739728 -> 1675296739920
	1675296739728 [label=ConvolutionBackward0]
	1675296741504 -> 1675296739728
	1675296741504 [label=NativeBatchNormBackward0]
	1675296706320 -> 1675296741504
	1675296706320 [label=ConvolutionBackward0]
	1675296706128 -> 1675296706320
	1675296706128 [label=MulBackward0]
	1675296705840 -> 1675296706128
	1675296705840 [label=SiluBackward0]
	1675296705552 -> 1675296705840
	1675296705552 [label=NativeBatchNormBackward0]
	1675296705360 -> 1675296705552
	1675296705360 [label=ConvolutionBackward0]
	1675296705168 -> 1675296705360
	1675296705168 [label=SiluBackward0]
	1675296704880 -> 1675296705168
	1675296704880 [label=NativeBatchNormBackward0]
	1675296704688 -> 1675296704880
	1675296704688 [label=ConvolutionBackward0]
	1675296704256 -> 1675296704688
	1675296704256 [label=AddBackward0]
	1675296704112 -> 1675296704256
	1675296704112 [label=NativeBatchNormBackward0]
	1675296703920 -> 1675296704112
	1675296703920 [label=ConvolutionBackward0]
	1675296703584 -> 1675296703920
	1675296703584 [label=MulBackward0]
	1675296703440 -> 1675296703584
	1675296703440 [label=SiluBackward0]
	1675296703152 -> 1675296703440
	1675296703152 [label=NativeBatchNormBackward0]
	1675296702960 -> 1675296703152
	1675296702960 [label=ConvolutionBackward0]
	1675296702768 -> 1675296702960
	1675296702768 [label=SiluBackward0]
	1675296702576 -> 1675296702768
	1675296702576 [label=NativeBatchNormBackward0]
	1675296681648 -> 1675296702576
	1675296681648 [label=ConvolutionBackward0]
	1675296704208 -> 1675296681648
	1675296704208 [label=AddBackward0]
	1675296681120 -> 1675296704208
	1675296681120 [label=NativeBatchNormBackward0]
	1675296680976 -> 1675296681120
	1675296680976 [label=ConvolutionBackward0]
	1675296680592 -> 1675296680976
	1675296680592 [label=MulBackward0]
	1675296680256 -> 1675296680592
	1675296680256 [label=SiluBackward0]
	1675296680112 -> 1675296680256
	1675296680112 [label=NativeBatchNormBackward0]
	1675296680016 -> 1675296680112
	1675296680016 [label=ConvolutionBackward0]
	1675296679728 -> 1675296680016
	1675296679728 [label=SiluBackward0]
	1675296679536 -> 1675296679728
	1675296679536 [label=NativeBatchNormBackward0]
	1675296679344 -> 1675296679536
	1675296679344 [label=ConvolutionBackward0]
	1675296681264 -> 1675296679344
	1675296681264 [label=AddBackward0]
	1675296678720 -> 1675296681264
	1675296678720 [label=NativeBatchNormBackward0]
	1675296678672 -> 1675296678720
	1675296678672 [label=ConvolutionBackward0]
	1675296678288 -> 1675296678672
	1675296678288 [label=MulBackward0]
	1675296678000 -> 1675296678288
	1675296678000 [label=SiluBackward0]
	1675296653168 -> 1675296678000
	1675296653168 [label=NativeBatchNormBackward0]
	1675296653072 -> 1675296653168
	1675296653072 [label=ConvolutionBackward0]
	1675296652688 -> 1675296653072
	1675296652688 [label=SiluBackward0]
	1675296652400 -> 1675296652688
	1675296652400 [label=NativeBatchNormBackward0]
	1675296652160 -> 1675296652400
	1675296652160 [label=ConvolutionBackward0]
	1675296678864 -> 1675296652160
	1675296678864 [label=AddBackward0]
	1675296651824 -> 1675296678864
	1675296651824 [label=NativeBatchNormBackward0]
	1675296651632 -> 1675296651824
	1675296651632 [label=ConvolutionBackward0]
	1675296651344 -> 1675296651632
	1675296651344 [label=MulBackward0]
	1675296651056 -> 1675296651344
	1675296651056 [label=SiluBackward0]
	1675296650768 -> 1675296651056
	1675296650768 [label=NativeBatchNormBackward0]
	1675296650672 -> 1675296650768
	1675296650672 [label=ConvolutionBackward0]
	1675296650384 -> 1675296650672
	1675296650384 [label=SiluBackward0]
	1675296650096 -> 1675296650384
	1675296650096 [label=NativeBatchNormBackward0]
	1675296649904 -> 1675296650096
	1675296649904 [label=ConvolutionBackward0]
	1675296651920 -> 1675296649904
	1675296651920 [label=AddBackward0]
	1675296649520 -> 1675296651920
	1675296649520 [label=NativeBatchNormBackward0]
	1675296649328 -> 1675296649520
	1675296649328 [label=ConvolutionBackward0]
	1675280039504 -> 1675296649328
	1675280039504 [label=MulBackward0]
	1675280039312 -> 1675280039504
	1675280039312 [label=SiluBackward0]
	1675280039120 -> 1675280039312
	1675280039120 [label=NativeBatchNormBackward0]
	1675280038928 -> 1675280039120
	1675280038928 [label=ConvolutionBackward0]
	1675280038736 -> 1675280038928
	1675280038736 [label=SiluBackward0]
	1675280038448 -> 1675280038736
	1675280038448 [label=NativeBatchNormBackward0]
	1675280038256 -> 1675280038448
	1675280038256 [label=ConvolutionBackward0]
	1675296649616 -> 1675280038256
	1675296649616 [label=NativeBatchNormBackward0]
	1675280037728 -> 1675296649616
	1675280037728 [label=ConvolutionBackward0]
	1675280037488 -> 1675280037728
	1675280037488 [label=MulBackward0]
	1675280037200 -> 1675280037488
	1675280037200 [label=SiluBackward0]
	1675280036864 -> 1675280037200
	1675280036864 [label=NativeBatchNormBackward0]
	1675280036768 -> 1675280036864
	1675280036768 [label=ConvolutionBackward0]
	1675280036528 -> 1675280036768
	1675280036528 [label=ConstantPadNdBackward0]
	1675280036240 -> 1675280036528
	1675280036240 [label=SiluBackward0]
	1675280036000 -> 1675280036240
	1675280036000 [label=NativeBatchNormBackward0]
	1675280035904 -> 1675280036000
	1675280035904 [label=ConvolutionBackward0]
	1675280006928 -> 1675280035904
	1675280006928 [label=AddBackward0]
	1675280006592 -> 1675280006928
	1675280006592 [label=NativeBatchNormBackward0]
	1675280006544 -> 1675280006592
	1675280006544 [label=ConvolutionBackward0]
	1675280006160 -> 1675280006544
	1675280006160 [label=SiluBackward0]
	1675280005872 -> 1675280006160
	1675280005872 [label=NativeBatchNormBackward0]
	1675280005632 -> 1675280005872
	1675280005632 [label=ConvolutionBackward0]
	1675280006736 -> 1675280005632
	1675280006736 [label=AddBackward0]
	1675280005296 -> 1675280006736
	1675280005296 [label=NativeBatchNormBackward0]
	1675280005008 -> 1675280005296
	1675280005008 [label=ConvolutionBackward0]
	1675280004720 -> 1675280005008
	1675280004720 [label=SiluBackward0]
	1675280004528 -> 1675280004720
	1675280004528 [label=NativeBatchNormBackward0]
	1675280004336 -> 1675280004528
	1675280004336 [label=ConvolutionBackward0]
	1675280005392 -> 1675280004336
	1675280005392 [label=AddBackward0]
	1675280003712 -> 1675280005392
	1675280003712 [label=NativeBatchNormBackward0]
	1675280003568 -> 1675280003712
	1675280003568 [label=ConvolutionBackward0]
	1675280003232 -> 1675280003568
	1675280003232 [label=SiluBackward0]
	1675280003184 -> 1675280003232
	1675280003184 [label=NativeBatchNormBackward0]
	1675280003280 -> 1675280003184
	1675280003280 [label=ConvolutionBackward0]
	1675280003856 -> 1675280003280
	1675280003856 [label=NativeBatchNormBackward0]
	1675279981872 -> 1675280003856
	1675279981872 [label=ConvolutionBackward0]
	1675279981536 -> 1675279981872
	1675279981536 [label=SiluBackward0]
	1675279981392 -> 1675279981536
	1675279981392 [label=NativeBatchNormBackward0]
	1675279981200 -> 1675279981392
	1675279981200 [label=ConvolutionBackward0]
	1675279980816 -> 1675279981200
	1675279980816 [label=ConstantPadNdBackward0]
	1675279980576 -> 1675279980816
	1675279980576 [label=AddBackward0]
	1675279980528 -> 1675279980576
	1675279980528 [label=NativeBatchNormBackward0]
	1675279980240 -> 1675279980528
	1675279980240 [label=ConvolutionBackward0]
	1675279979856 -> 1675279980240
	1675279979856 [label=SiluBackward0]
	1675279979664 -> 1675279979856
	1675279979664 [label=NativeBatchNormBackward0]
	1675279979424 -> 1675279979664
	1675279979424 [label=ConvolutionBackward0]
	1675279980624 -> 1675279979424
	1675279980624 [label=AddBackward0]
	1675279979040 -> 1675279980624
	1675279979040 [label=NativeBatchNormBackward0]
	1675279978896 -> 1675279979040
	1675279978896 [label=ConvolutionBackward0]
	1675279978608 -> 1675279978896
	1675279978608 [label=SiluBackward0]
	1675279945344 -> 1675279978608
	1675279945344 [label=NativeBatchNormBackward0]
	1675279945248 -> 1675279945344
	1675279945248 [label=ConvolutionBackward0]
	1675279979184 -> 1675279945248
	1675279979184 [label=AddBackward0]
	1675279944816 -> 1675279979184
	1675279944816 [label=NativeBatchNormBackward0]
	1675279944528 -> 1675279944816
	1675279944528 [label=ConvolutionBackward0]
	1675279944336 -> 1675279944528
	1675279944336 [label=SiluBackward0]
	1675279944048 -> 1675279944336
	1675279944048 [label=NativeBatchNormBackward0]
	1675279943856 -> 1675279944048
	1675279943856 [label=ConvolutionBackward0]
	1675279944912 -> 1675279943856
	1675279944912 [label=NativeBatchNormBackward0]
	1675279943376 -> 1675279944912
	1675279943376 [label=ConvolutionBackward0]
	1675279943088 -> 1675279943376
	1675279943088 [label=SiluBackward0]
	1675279942896 -> 1675279943088
	1675279942896 [label=NativeBatchNormBackward0]
	1675279942800 -> 1675279942896
	1675279942800 [label=ConvolutionBackward0]
	1675279942416 -> 1675279942800
	1675279942416 [label=ConstantPadNdBackward0]
	1675279942128 -> 1675279942416
	1675279942128 [label=AddBackward0]
	1675279941888 -> 1675279942128
	1675279941888 [label=SiluBackward0]
	1675279941840 -> 1675279941888
	1675279941840 [label=NativeBatchNormBackward0]
	1675279941744 -> 1675279941840
	1675279941744 [label=ConvolutionBackward0]
	1675279942032 -> 1675279941744
	1675279942032 [label=AddBackward0]
	1675279916432 -> 1675279942032
	1675279916432 [label=SiluBackward0]
	1675279916192 -> 1675279916432
	1675279916192 [label=NativeBatchNormBackward0]
	1675279916144 -> 1675279916192
	1675279916144 [label=ConvolutionBackward0]
	1675279916528 -> 1675279916144
	1675279916528 [label=SiluBackward0]
	1675279915568 -> 1675279916528
	1675279915568 [label=NativeBatchNormBackward0]
	1675279915328 -> 1675279915568
	1675279915328 [label=ConvolutionBackward0]
	1675279915040 -> 1675279915328
	1675275269280 [label="conv_stem.weight
 (24, 3, 3, 3)" fillcolor=lightblue]
	1675275269280 -> 1675279915040
	1675279915040 [label=AccumulateGrad]
	1675279915472 -> 1675279915568
	1675275204624 [label="bn1.weight
 (24)" fillcolor=lightblue]
	1675275204624 -> 1675279915472
	1675279915472 [label=AccumulateGrad]
	1675279915952 -> 1675279915568
	1675274538496 [label="bn1.bias
 (24)" fillcolor=lightblue]
	1675274538496 -> 1675279915952
	1675279915952 [label=AccumulateGrad]
	1675279915760 -> 1675279916144
	1675275204464 [label="blocks.0.0.conv.weight
 (24, 24, 3, 3)" fillcolor=lightblue]
	1675275204464 -> 1675279915760
	1675279915760 [label=AccumulateGrad]
	1675279916240 -> 1675279916192
	1675274777120 [label="blocks.0.0.bn1.weight
 (24)" fillcolor=lightblue]
	1675274777120 -> 1675279916240
	1675279916240 [label=AccumulateGrad]
	1675279916288 -> 1675279916192
	1675275366704 [label="blocks.0.0.bn1.bias
 (24)" fillcolor=lightblue]
	1675275366704 -> 1675279916288
	1675279916288 [label=AccumulateGrad]
	1675279916528 -> 1675279942032
	1675279916624 -> 1675279941744
	1675275370224 [label="blocks.0.1.conv.weight
 (24, 24, 3, 3)" fillcolor=lightblue]
	1675275370224 -> 1675279916624
	1675279916624 [label=AccumulateGrad]
	1675279941936 -> 1675279941840
	1675275369344 [label="blocks.0.1.bn1.weight
 (24)" fillcolor=lightblue]
	1675275369344 -> 1675279941936
	1675279941936 [label=AccumulateGrad]
	1675279917008 -> 1675279941840
	1675275368704 [label="blocks.0.1.bn1.bias
 (24)" fillcolor=lightblue]
	1675275368704 -> 1675279917008
	1675279917008 [label=AccumulateGrad]
	1675279942032 -> 1675279942128
	1675279942512 -> 1675279942800
	1675275367664 [label="blocks.1.0.conv_exp.weight
 (96, 24, 3, 3)" fillcolor=lightblue]
	1675275367664 -> 1675279942512
	1675279942512 [label=AccumulateGrad]
	1675279942752 -> 1675279942896
	1675275369424 [label="blocks.1.0.bn1.weight
 (96)" fillcolor=lightblue]
	1675275369424 -> 1675279942752
	1675279942752 [label=AccumulateGrad]
	1675279942992 -> 1675279942896
	1675275366464 [label="blocks.1.0.bn1.bias
 (96)" fillcolor=lightblue]
	1675275366464 -> 1675279942992
	1675279942992 [label=AccumulateGrad]
	1675279943184 -> 1675279943376
	1675275367984 [label="blocks.1.0.conv_pwl.weight
 (48, 96, 1, 1)" fillcolor=lightblue]
	1675275367984 -> 1675279943184
	1675279943184 [label=AccumulateGrad]
	1675279943472 -> 1675279944912
	1675275367744 [label="blocks.1.0.bn2.weight
 (48)" fillcolor=lightblue]
	1675275367744 -> 1675279943472
	1675279943472 [label=AccumulateGrad]
	1675279943664 -> 1675279944912
	1675275369744 [label="blocks.1.0.bn2.bias
 (48)" fillcolor=lightblue]
	1675275369744 -> 1675279943664
	1675279943664 [label=AccumulateGrad]
	1675279943424 -> 1675279943856
	1675275368384 [label="blocks.1.1.conv_exp.weight
 (192, 48, 3, 3)" fillcolor=lightblue]
	1675275368384 -> 1675279943424
	1675279943424 [label=AccumulateGrad]
	1675279943952 -> 1675279944048
	1675275369584 [label="blocks.1.1.bn1.weight
 (192)" fillcolor=lightblue]
	1675275369584 -> 1675279943952
	1675279943952 [label=AccumulateGrad]
	1675279944240 -> 1675279944048
	1675275366624 [label="blocks.1.1.bn1.bias
 (192)" fillcolor=lightblue]
	1675275366624 -> 1675279944240
	1675279944240 [label=AccumulateGrad]
	1675279944288 -> 1675279944528
	1675275370064 [label="blocks.1.1.conv_pwl.weight
 (48, 192, 1, 1)" fillcolor=lightblue]
	1675275370064 -> 1675279944288
	1675279944288 [label=AccumulateGrad]
	1675279944624 -> 1675279944816
	1675275370144 [label="blocks.1.1.bn2.weight
 (48)" fillcolor=lightblue]
	1675275370144 -> 1675279944624
	1675279944624 [label=AccumulateGrad]
	1675279944720 -> 1675279944816
	1675275370304 [label="blocks.1.1.bn2.bias
 (48)" fillcolor=lightblue]
	1675275370304 -> 1675279944720
	1675279944720 [label=AccumulateGrad]
	1675279944912 -> 1675279979184
	1675279945008 -> 1675279945248
	1675275755904 [label="blocks.1.2.conv_exp.weight
 (192, 48, 3, 3)" fillcolor=lightblue]
	1675275755904 -> 1675279945008
	1675279945008 [label=AccumulateGrad]
	1675279945392 -> 1675279945344
	1675275755824 [label="blocks.1.2.bn1.weight
 (192)" fillcolor=lightblue]
	1675275755824 -> 1675279945392
	1675279945392 [label=AccumulateGrad]
	1675279945584 -> 1675279945344
	1675275755984 [label="blocks.1.2.bn1.bias
 (192)" fillcolor=lightblue]
	1675275755984 -> 1675279945584
	1675279945584 [label=AccumulateGrad]
	1675279978704 -> 1675279978896
	1675275756384 [label="blocks.1.2.conv_pwl.weight
 (48, 192, 1, 1)" fillcolor=lightblue]
	1675275756384 -> 1675279978704
	1675279978704 [label=AccumulateGrad]
	1675279978992 -> 1675279979040
	1675275756464 [label="blocks.1.2.bn2.weight
 (48)" fillcolor=lightblue]
	1675275756464 -> 1675279978992
	1675279978992 [label=AccumulateGrad]
	1675279979088 -> 1675279979040
	1675275756544 [label="blocks.1.2.bn2.bias
 (48)" fillcolor=lightblue]
	1675275756544 -> 1675279979088
	1675279979088 [label=AccumulateGrad]
	1675279979184 -> 1675279980624
	1675279979136 -> 1675279979424
	1675275757024 [label="blocks.1.3.conv_exp.weight
 (192, 48, 3, 3)" fillcolor=lightblue]
	1675275757024 -> 1675279979136
	1675279979136 [label=AccumulateGrad]
	1675279979568 -> 1675279979664
	1675275756944 [label="blocks.1.3.bn1.weight
 (192)" fillcolor=lightblue]
	1675275756944 -> 1675279979568
	1675279979568 [label=AccumulateGrad]
	1675279979712 -> 1675279979664
	1675275757104 [label="blocks.1.3.bn1.bias
 (192)" fillcolor=lightblue]
	1675275757104 -> 1675279979712
	1675279979712 [label=AccumulateGrad]
	1675279979952 -> 1675279980240
	1675275757504 [label="blocks.1.3.conv_pwl.weight
 (48, 192, 1, 1)" fillcolor=lightblue]
	1675275757504 -> 1675279979952
	1675279979952 [label=AccumulateGrad]
	1675279980336 -> 1675279980528
	1675275757584 [label="blocks.1.3.bn2.weight
 (48)" fillcolor=lightblue]
	1675275757584 -> 1675279980336
	1675279980336 [label=AccumulateGrad]
	1675279980432 -> 1675279980528
	1675275757664 [label="blocks.1.3.bn2.bias
 (48)" fillcolor=lightblue]
	1675275757664 -> 1675279980432
	1675279980432 [label=AccumulateGrad]
	1675279980624 -> 1675279980576
	1675279980912 -> 1675279981200
	1675275758144 [label="blocks.2.0.conv_exp.weight
 (192, 48, 3, 3)" fillcolor=lightblue]
	1675275758144 -> 1675279980912
	1675279980912 [label=AccumulateGrad]
	1675279981296 -> 1675279981392
	1675275758064 [label="blocks.2.0.bn1.weight
 (192)" fillcolor=lightblue]
	1675275758064 -> 1675279981296
	1675279981296 [label=AccumulateGrad]
	1675279981584 -> 1675279981392
	1675275758224 [label="blocks.2.0.bn1.bias
 (192)" fillcolor=lightblue]
	1675275758224 -> 1675279981584
	1675279981584 [label=AccumulateGrad]
	1675279981680 -> 1675279981872
	1675275758624 [label="blocks.2.0.conv_pwl.weight
 (64, 192, 1, 1)" fillcolor=lightblue]
	1675275758624 -> 1675279981680
	1675279981680 [label=AccumulateGrad]
	1675279981968 -> 1675280003856
	1675275758704 [label="blocks.2.0.bn2.weight
 (64)" fillcolor=lightblue]
	1675275758704 -> 1675279981968
	1675279981968 [label=AccumulateGrad]
	1675279982256 -> 1675280003856
	1675275758784 [label="blocks.2.0.bn2.bias
 (64)" fillcolor=lightblue]
	1675275758784 -> 1675279982256
	1675279982256 [label=AccumulateGrad]
	1675279982064 -> 1675280003280
	1675275759264 [label="blocks.2.1.conv_exp.weight
 (256, 64, 3, 3)" fillcolor=lightblue]
	1675275759264 -> 1675279982064
	1675279982064 [label=AccumulateGrad]
	1675279982448 -> 1675280003184
	1675275759184 [label="blocks.2.1.bn1.weight
 (256)" fillcolor=lightblue]
	1675275759184 -> 1675279982448
	1675279982448 [label=AccumulateGrad]
	1675279982544 -> 1675280003184
	1675275759344 [label="blocks.2.1.bn1.bias
 (256)" fillcolor=lightblue]
	1675275759344 -> 1675279982544
	1675279982544 [label=AccumulateGrad]
	1675280003376 -> 1675280003568
	1675276005600 [label="blocks.2.1.conv_pwl.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	1675276005600 -> 1675280003376
	1675280003376 [label=AccumulateGrad]
	1675280003664 -> 1675280003712
	1675276005680 [label="blocks.2.1.bn2.weight
 (64)" fillcolor=lightblue]
	1675276005680 -> 1675280003664
	1675280003664 [label=AccumulateGrad]
	1675280003760 -> 1675280003712
	1675276005760 [label="blocks.2.1.bn2.bias
 (64)" fillcolor=lightblue]
	1675276005760 -> 1675280003760
	1675280003760 [label=AccumulateGrad]
	1675280003856 -> 1675280005392
	1675280003952 -> 1675280004336
	1675276006240 [label="blocks.2.2.conv_exp.weight
 (256, 64, 3, 3)" fillcolor=lightblue]
	1675276006240 -> 1675280003952
	1675280003952 [label=AccumulateGrad]
	1675280004432 -> 1675280004528
	1675276006160 [label="blocks.2.2.bn1.weight
 (256)" fillcolor=lightblue]
	1675276006160 -> 1675280004432
	1675280004432 [label=AccumulateGrad]
	1675280004576 -> 1675280004528
	1675276006320 [label="blocks.2.2.bn1.bias
 (256)" fillcolor=lightblue]
	1675276006320 -> 1675280004576
	1675280004576 [label=AccumulateGrad]
	1675280004672 -> 1675280005008
	1675276006720 [label="blocks.2.2.conv_pwl.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	1675276006720 -> 1675280004672
	1675280004672 [label=AccumulateGrad]
	1675280005104 -> 1675280005296
	1675276006800 [label="blocks.2.2.bn2.weight
 (64)" fillcolor=lightblue]
	1675276006800 -> 1675280005104
	1675280005104 [label=AccumulateGrad]
	1675280005200 -> 1675280005296
	1675276006880 [label="blocks.2.2.bn2.bias
 (64)" fillcolor=lightblue]
	1675276006880 -> 1675280005200
	1675280005200 [label=AccumulateGrad]
	1675280005392 -> 1675280006736
	1675280005488 -> 1675280005632
	1675276007360 [label="blocks.2.3.conv_exp.weight
 (256, 64, 3, 3)" fillcolor=lightblue]
	1675276007360 -> 1675280005488
	1675280005488 [label=AccumulateGrad]
	1675280005776 -> 1675280005872
	1675276007280 [label="blocks.2.3.bn1.weight
 (256)" fillcolor=lightblue]
	1675276007280 -> 1675280005776
	1675280005776 [label=AccumulateGrad]
	1675280006064 -> 1675280005872
	1675276007440 [label="blocks.2.3.bn1.bias
 (256)" fillcolor=lightblue]
	1675276007440 -> 1675280006064
	1675280006064 [label=AccumulateGrad]
	1675280006256 -> 1675280006544
	1675276007840 [label="blocks.2.3.conv_pwl.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	1675276007840 -> 1675280006256
	1675280006256 [label=AccumulateGrad]
	1675280006496 -> 1675280006592
	1675276007920 [label="blocks.2.3.bn2.weight
 (64)" fillcolor=lightblue]
	1675276007920 -> 1675280006496
	1675280006496 [label=AccumulateGrad]
	1675280006640 -> 1675280006592
	1675276008000 [label="blocks.2.3.bn2.bias
 (64)" fillcolor=lightblue]
	1675276008000 -> 1675280006640
	1675280006640 [label=AccumulateGrad]
	1675280006736 -> 1675280006928
	1675280006880 -> 1675280035904
	1675276008240 [label="blocks.3.0.conv_pw.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	1675276008240 -> 1675280006880
	1675280006880 [label=AccumulateGrad]
	1675280036048 -> 1675280036000
	1675276008320 [label="blocks.3.0.bn1.weight
 (256)" fillcolor=lightblue]
	1675276008320 -> 1675280036048
	1675280036048 [label=AccumulateGrad]
	1675280036432 -> 1675280036000
	1675276008400 [label="blocks.3.0.bn1.bias
 (256)" fillcolor=lightblue]
	1675276008400 -> 1675280036432
	1675280036432 [label=AccumulateGrad]
	1675280036624 -> 1675280036768
	1675276008880 [label="blocks.3.0.conv_dw.weight
 (256, 1, 3, 3)" fillcolor=lightblue]
	1675276008880 -> 1675280036624
	1675280036624 [label=AccumulateGrad]
	1675280036912 -> 1675280036864
	1675276008800 [label="blocks.3.0.bn2.weight
 (256)" fillcolor=lightblue]
	1675276008800 -> 1675280036912
	1675280036912 [label=AccumulateGrad]
	1675280037104 -> 1675280036864
	1675276008960 [label="blocks.3.0.bn2.bias
 (256)" fillcolor=lightblue]
	1675276008960 -> 1675280037104
	1675280037104 [label=AccumulateGrad]
	1675280037296 -> 1675280037488
	1675280037296 [label=SigmoidBackward0]
	1675280036720 -> 1675280037296
	1675280036720 [label=ConvolutionBackward0]
	1675280036144 -> 1675280036720
	1675280036144 [label=SiluBackward0]
	1675280006448 -> 1675280036144
	1675280006448 [label=ConvolutionBackward0]
	1675280005584 -> 1675280006448
	1675280005584 [label=MeanBackward1]
	1675280037200 -> 1675280005584
	1675280006832 -> 1675280006448
	1675276009360 [label="blocks.3.0.se.conv_reduce.weight
 (16, 256, 1, 1)" fillcolor=lightblue]
	1675276009360 -> 1675280006832
	1675280006832 [label=AccumulateGrad]
	1675280007024 -> 1675280006448
	1675276222528 [label="blocks.3.0.se.conv_reduce.bias
 (16)" fillcolor=lightblue]
	1675276222528 -> 1675280007024
	1675280007024 [label=AccumulateGrad]
	1675280035952 -> 1675280036720
	1675276222688 [label="blocks.3.0.se.conv_expand.weight
 (256, 16, 1, 1)" fillcolor=lightblue]
	1675276222688 -> 1675280035952
	1675280035952 [label=AccumulateGrad]
	1675280037008 -> 1675280036720
	1675276222768 [label="blocks.3.0.se.conv_expand.bias
 (256)" fillcolor=lightblue]
	1675276222768 -> 1675280037008
	1675280037008 [label=AccumulateGrad]
	1675280037584 -> 1675280037728
	1675276222928 [label="blocks.3.0.conv_pwl.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	1675276222928 -> 1675280037584
	1675280037584 [label=AccumulateGrad]
	1675280037872 -> 1675296649616
	1675276223008 [label="blocks.3.0.bn3.weight
 (128)" fillcolor=lightblue]
	1675276223008 -> 1675280037872
	1675280037872 [label=AccumulateGrad]
	1675280038064 -> 1675296649616
	1675276223088 [label="blocks.3.0.bn3.bias
 (128)" fillcolor=lightblue]
	1675276223088 -> 1675280038064
	1675280038064 [label=AccumulateGrad]
	1675280037824 -> 1675280038256
	1675276223568 [label="blocks.3.1.conv_pw.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1675276223568 -> 1675280037824
	1675280037824 [label=AccumulateGrad]
	1675280038352 -> 1675280038448
	1675276223648 [label="blocks.3.1.bn1.weight
 (512)" fillcolor=lightblue]
	1675276223648 -> 1675280038352
	1675280038352 [label=AccumulateGrad]
	1675280038640 -> 1675280038448
	1675276223728 [label="blocks.3.1.bn1.bias
 (512)" fillcolor=lightblue]
	1675276223728 -> 1675280038640
	1675280038640 [label=AccumulateGrad]
	1675280038688 -> 1675280038928
	1675276224208 [label="blocks.3.1.conv_dw.weight
 (512, 1, 3, 3)" fillcolor=lightblue]
	1675276224208 -> 1675280038688
	1675280038688 [label=AccumulateGrad]
	1675280039024 -> 1675280039120
	1675276224128 [label="blocks.3.1.bn2.weight
 (512)" fillcolor=lightblue]
	1675276224128 -> 1675280039024
	1675280039024 [label=AccumulateGrad]
	1675280039216 -> 1675280039120
	1675276224288 [label="blocks.3.1.bn2.bias
 (512)" fillcolor=lightblue]
	1675276224288 -> 1675280039216
	1675280039216 [label=AccumulateGrad]
	1675280039408 -> 1675280039504
	1675280039408 [label=SigmoidBackward0]
	1675280038832 -> 1675280039408
	1675280038832 [label=ConvolutionBackward0]
	1675280038160 -> 1675280038832
	1675280038160 [label=SiluBackward0]
	1675280037776 -> 1675280038160
	1675280037776 [label=ConvolutionBackward0]
	1675280037392 -> 1675280037776
	1675280037392 [label=MeanBackward1]
	1675280039312 -> 1675280037392
	1675280036816 -> 1675280037776
	1675276224688 [label="blocks.3.1.se.conv_reduce.weight
 (32, 512, 1, 1)" fillcolor=lightblue]
	1675276224688 -> 1675280036816
	1675280036816 [label=AccumulateGrad]
	1675280036336 -> 1675280037776
	1675276224768 [label="blocks.3.1.se.conv_reduce.bias
 (32)" fillcolor=lightblue]
	1675276224768 -> 1675280036336
	1675280036336 [label=AccumulateGrad]
	1675280037968 -> 1675280038832
	1675276224928 [label="blocks.3.1.se.conv_expand.weight
 (512, 32, 1, 1)" fillcolor=lightblue]
	1675276224928 -> 1675280037968
	1675280037968 [label=AccumulateGrad]
	1675280039072 -> 1675280038832
	1675276225008 [label="blocks.3.1.se.conv_expand.bias
 (512)" fillcolor=lightblue]
	1675276225008 -> 1675280039072
	1675280039072 [label=AccumulateGrad]
	1675280039600 -> 1675296649328
	1675276225168 [label="blocks.3.1.conv_pwl.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	1675276225168 -> 1675280039600
	1675280039600 [label=AccumulateGrad]
	1675296649424 -> 1675296649520
	1675276225248 [label="blocks.3.1.bn3.weight
 (128)" fillcolor=lightblue]
	1675276225248 -> 1675296649424
	1675296649424 [label=AccumulateGrad]
	1675280039888 -> 1675296649520
	1675276225328 [label="blocks.3.1.bn3.bias
 (128)" fillcolor=lightblue]
	1675276225328 -> 1675280039888
	1675280039888 [label=AccumulateGrad]
	1675296649616 -> 1675296651920
	1675296649568 -> 1675296649904
	1675276225808 [label="blocks.3.2.conv_pw.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1675276225808 -> 1675296649568
	1675296649568 [label=AccumulateGrad]
	1675296650000 -> 1675296650096
	1675276225888 [label="blocks.3.2.bn1.weight
 (512)" fillcolor=lightblue]
	1675276225888 -> 1675296650000
	1675296650000 [label=AccumulateGrad]
	1675296650288 -> 1675296650096
	1675276225968 [label="blocks.3.2.bn1.bias
 (512)" fillcolor=lightblue]
	1675276225968 -> 1675296650288
	1675296650288 [label=AccumulateGrad]
	1675296650480 -> 1675296650672
	1675276226448 [label="blocks.3.2.conv_dw.weight
 (512, 1, 3, 3)" fillcolor=lightblue]
	1675276226448 -> 1675296650480
	1675296650480 [label=AccumulateGrad]
	1675296650624 -> 1675296650768
	1675276226368 [label="blocks.3.2.bn2.weight
 (512)" fillcolor=lightblue]
	1675276226368 -> 1675296650624
	1675296650624 [label=AccumulateGrad]
	1675296650960 -> 1675296650768
	1675276398656 [label="blocks.3.2.bn2.bias
 (512)" fillcolor=lightblue]
	1675276398656 -> 1675296650960
	1675296650960 [label=AccumulateGrad]
	1675296651152 -> 1675296651344
	1675296651152 [label=SigmoidBackward0]
	1675296650576 -> 1675296651152
	1675296650576 [label=ConvolutionBackward0]
	1675296649808 -> 1675296650576
	1675296649808 [label=SiluBackward0]
	1675296649664 -> 1675296649808
	1675296649664 [label=ConvolutionBackward0]
	1675280038784 -> 1675296649664
	1675280038784 [label=MeanBackward1]
	1675296651056 -> 1675280038784
	1675280038544 -> 1675296649664
	1675276399056 [label="blocks.3.2.se.conv_reduce.weight
 (32, 512, 1, 1)" fillcolor=lightblue]
	1675276399056 -> 1675280038544
	1675280038544 [label=AccumulateGrad]
	1675280039696 -> 1675296649664
	1675276399136 [label="blocks.3.2.se.conv_reduce.bias
 (32)" fillcolor=lightblue]
	1675276399136 -> 1675280039696
	1675280039696 [label=AccumulateGrad]
	1675296649712 -> 1675296650576
	1675276399296 [label="blocks.3.2.se.conv_expand.weight
 (512, 32, 1, 1)" fillcolor=lightblue]
	1675276399296 -> 1675296649712
	1675296649712 [label=AccumulateGrad]
	1675296650864 -> 1675296650576
	1675276399376 [label="blocks.3.2.se.conv_expand.bias
 (512)" fillcolor=lightblue]
	1675276399376 -> 1675296650864
	1675296650864 [label=AccumulateGrad]
	1675296651440 -> 1675296651632
	1675276399536 [label="blocks.3.2.conv_pwl.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	1675276399536 -> 1675296651440
	1675296651440 [label=AccumulateGrad]
	1675296651584 -> 1675296651824
	1675276399616 [label="blocks.3.2.bn3.weight
 (128)" fillcolor=lightblue]
	1675276399616 -> 1675296651584
	1675296651584 [label=AccumulateGrad]
	1675296651728 -> 1675296651824
	1675276399696 [label="blocks.3.2.bn3.bias
 (128)" fillcolor=lightblue]
	1675276399696 -> 1675296651728
	1675296651728 [label=AccumulateGrad]
	1675296651920 -> 1675296678864
	1675296651872 -> 1675296652160
	1675276400176 [label="blocks.3.3.conv_pw.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1675276400176 -> 1675296651872
	1675296651872 [label=AccumulateGrad]
	1675296652304 -> 1675296652400
	1675276400256 [label="blocks.3.3.bn1.weight
 (512)" fillcolor=lightblue]
	1675276400256 -> 1675296652304
	1675296652304 [label=AccumulateGrad]
	1675296652592 -> 1675296652400
	1675276400336 [label="blocks.3.3.bn1.bias
 (512)" fillcolor=lightblue]
	1675276400336 -> 1675296652592
	1675296652592 [label=AccumulateGrad]
	1675296652784 -> 1675296653072
	1675276400816 [label="blocks.3.3.conv_dw.weight
 (512, 1, 3, 3)" fillcolor=lightblue]
	1675276400816 -> 1675296652784
	1675296652784 [label=AccumulateGrad]
	1675296653024 -> 1675296653168
	1675276400736 [label="blocks.3.3.bn2.weight
 (512)" fillcolor=lightblue]
	1675276400736 -> 1675296653024
	1675296653024 [label=AccumulateGrad]
	1675296653264 -> 1675296653168
	1675276400896 [label="blocks.3.3.bn2.bias
 (512)" fillcolor=lightblue]
	1675276400896 -> 1675296653264
	1675296653264 [label=AccumulateGrad]
	1675296678096 -> 1675296678288
	1675296678096 [label=SigmoidBackward0]
	1675296652880 -> 1675296678096
	1675296652880 [label=ConvolutionBackward0]
	1675296652208 -> 1675296652880
	1675296652208 [label=SiluBackward0]
	1675296652112 -> 1675296652208
	1675296652112 [label=ConvolutionBackward0]
	1675296650528 -> 1675296652112
	1675296650528 [label=MeanBackward1]
	1675296678000 -> 1675296650528
	1675296650192 -> 1675296652112
	1675276401296 [label="blocks.3.3.se.conv_reduce.weight
 (32, 512, 1, 1)" fillcolor=lightblue]
	1675276401296 -> 1675296650192
	1675296650192 [label=AccumulateGrad]
	1675296651536 -> 1675296652112
	1675276401376 [label="blocks.3.3.se.conv_reduce.bias
 (32)" fillcolor=lightblue]
	1675276401376 -> 1675296651536
	1675296651536 [label=AccumulateGrad]
	1675296652016 -> 1675296652880
	1675276401536 [label="blocks.3.3.se.conv_expand.weight
 (512, 32, 1, 1)" fillcolor=lightblue]
	1675276401536 -> 1675296652016
	1675296652016 [label=AccumulateGrad]
	1675296653120 -> 1675296652880
	1675276401616 [label="blocks.3.3.se.conv_expand.bias
 (512)" fillcolor=lightblue]
	1675276401616 -> 1675296653120
	1675296653120 [label=AccumulateGrad]
	1675296678384 -> 1675296678672
	1675276401776 [label="blocks.3.3.conv_pwl.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	1675276401776 -> 1675296678384
	1675296678384 [label=AccumulateGrad]
	1675296678624 -> 1675296678720
	1675276401856 [label="blocks.3.3.bn3.weight
 (128)" fillcolor=lightblue]
	1675276401856 -> 1675296678624
	1675296678624 [label=AccumulateGrad]
	1675296678768 -> 1675296678720
	1675276401936 [label="blocks.3.3.bn3.bias
 (128)" fillcolor=lightblue]
	1675276401936 -> 1675296678768
	1675296678768 [label=AccumulateGrad]
	1675296678864 -> 1675296681264
	1675296678960 -> 1675296679344
	1675276402416 [label="blocks.3.4.conv_pw.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1675276402416 -> 1675296678960
	1675296678960 [label=AccumulateGrad]
	1675296679440 -> 1675296679536
	1675276402496 [label="blocks.3.4.bn1.weight
 (512)" fillcolor=lightblue]
	1675276402496 -> 1675296679440
	1675296679440 [label=AccumulateGrad]
	1675296679584 -> 1675296679536
	1675276402576 [label="blocks.3.4.bn1.bias
 (512)" fillcolor=lightblue]
	1675276402576 -> 1675296679584
	1675296679584 [label=AccumulateGrad]
	1675296679680 -> 1675296680016
	1675276595664 [label="blocks.3.4.conv_dw.weight
 (512, 1, 3, 3)" fillcolor=lightblue]
	1675276595664 -> 1675296679680
	1675296679680 [label=AccumulateGrad]
	1675296679968 -> 1675296680112
	1675276595584 [label="blocks.3.4.bn2.weight
 (512)" fillcolor=lightblue]
	1675276595584 -> 1675296679968
	1675296679968 [label=AccumulateGrad]
	1675296680304 -> 1675296680112
	1675276595744 [label="blocks.3.4.bn2.bias
 (512)" fillcolor=lightblue]
	1675276595744 -> 1675296680304
	1675296680304 [label=AccumulateGrad]
	1675296680400 -> 1675296680592
	1675296680400 [label=SigmoidBackward0]
	1675296679824 -> 1675296680400
	1675296679824 [label=ConvolutionBackward0]
	1675296679248 -> 1675296679824
	1675296679248 [label=SiluBackward0]
	1675296679152 -> 1675296679248
	1675296679152 [label=ConvolutionBackward0]
	1675296678192 -> 1675296679152
	1675296678192 [label=MeanBackward1]
	1675296680256 -> 1675296678192
	1675296678480 -> 1675296679152
	1675276596144 [label="blocks.3.4.se.conv_reduce.weight
 (32, 512, 1, 1)" fillcolor=lightblue]
	1675276596144 -> 1675296678480
	1675296678480 [label=AccumulateGrad]
	1675296652496 -> 1675296679152
	1675276596224 [label="blocks.3.4.se.conv_reduce.bias
 (32)" fillcolor=lightblue]
	1675276596224 -> 1675296652496
	1675296652496 [label=AccumulateGrad]
	1675296679056 -> 1675296679824
	1675276596384 [label="blocks.3.4.se.conv_expand.weight
 (512, 32, 1, 1)" fillcolor=lightblue]
	1675276596384 -> 1675296679056
	1675296679056 [label=AccumulateGrad]
	1675296680208 -> 1675296679824
	1675276596464 [label="blocks.3.4.se.conv_expand.bias
 (512)" fillcolor=lightblue]
	1675276596464 -> 1675296680208
	1675296680208 [label=AccumulateGrad]
	1675296680688 -> 1675296680976
	1675276596624 [label="blocks.3.4.conv_pwl.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	1675276596624 -> 1675296680688
	1675296680688 [label=AccumulateGrad]
	1675296681072 -> 1675296681120
	1675276596704 [label="blocks.3.4.bn3.weight
 (128)" fillcolor=lightblue]
	1675276596704 -> 1675296681072
	1675296681072 [label=AccumulateGrad]
	1675296681168 -> 1675296681120
	1675276596784 [label="blocks.3.4.bn3.bias
 (128)" fillcolor=lightblue]
	1675276596784 -> 1675296681168
	1675296681168 [label=AccumulateGrad]
	1675296681264 -> 1675296704208
	1675296681216 -> 1675296681648
	1675276597264 [label="blocks.3.5.conv_pw.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	1675276597264 -> 1675296681216
	1675296681216 [label=AccumulateGrad]
	1675296681744 -> 1675296702576
	1675276597344 [label="blocks.3.5.bn1.weight
 (512)" fillcolor=lightblue]
	1675276597344 -> 1675296681744
	1675296681744 [label=AccumulateGrad]
	1675296681936 -> 1675296702576
	1675276597424 [label="blocks.3.5.bn1.bias
 (512)" fillcolor=lightblue]
	1675276597424 -> 1675296681936
	1675296681936 [label=AccumulateGrad]
	1675296702720 -> 1675296702960
	1675276597904 [label="blocks.3.5.conv_dw.weight
 (512, 1, 3, 3)" fillcolor=lightblue]
	1675276597904 -> 1675296702720
	1675296702720 [label=AccumulateGrad]
	1675296703056 -> 1675296703152
	1675276597824 [label="blocks.3.5.bn2.weight
 (512)" fillcolor=lightblue]
	1675276597824 -> 1675296703056
	1675296703056 [label=AccumulateGrad]
	1675296703344 -> 1675296703152
	1675276597984 [label="blocks.3.5.bn2.bias
 (512)" fillcolor=lightblue]
	1675276597984 -> 1675296703344
	1675296703344 [label=AccumulateGrad]
	1675296703536 -> 1675296703584
	1675296703536 [label=SigmoidBackward0]
	1675296702672 -> 1675296703536
	1675296702672 [label=ConvolutionBackward0]
	1675296703248 -> 1675296702672
	1675296703248 [label=SiluBackward0]
	1675296681456 -> 1675296703248
	1675296681456 [label=ConvolutionBackward0]
	1675296679920 -> 1675296681456
	1675296679920 [label=MeanBackward1]
	1675296703440 -> 1675296679920
	1675296679632 -> 1675296681456
	1675276598384 [label="blocks.3.5.se.conv_reduce.weight
 (32, 512, 1, 1)" fillcolor=lightblue]
	1675276598384 -> 1675296679632
	1675296679632 [label=AccumulateGrad]
	1675296680784 -> 1675296681456
	1675276598464 [label="blocks.3.5.se.conv_reduce.bias
 (32)" fillcolor=lightblue]
	1675276598464 -> 1675296680784
	1675296680784 [label=AccumulateGrad]
	1675296681552 -> 1675296702672
	1675276598624 [label="blocks.3.5.se.conv_expand.weight
 (512, 32, 1, 1)" fillcolor=lightblue]
	1675276598624 -> 1675296681552
	1675296681552 [label=AccumulateGrad]
	1675296681360 -> 1675296702672
	1675276598704 [label="blocks.3.5.se.conv_expand.bias
 (512)" fillcolor=lightblue]
	1675276598704 -> 1675296681360
	1675296681360 [label=AccumulateGrad]
	1675296703728 -> 1675296703920
	1675276598864 [label="blocks.3.5.conv_pwl.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	1675276598864 -> 1675296703728
	1675296703728 [label=AccumulateGrad]
	1675296704016 -> 1675296704112
	1675276598944 [label="blocks.3.5.bn3.weight
 (128)" fillcolor=lightblue]
	1675276598944 -> 1675296704016
	1675296704016 [label=AccumulateGrad]
	1675296703968 -> 1675296704112
	1675276599024 [label="blocks.3.5.bn3.bias
 (128)" fillcolor=lightblue]
	1675276599024 -> 1675296703968
	1675296703968 [label=AccumulateGrad]
	1675296704208 -> 1675296704256
	1675296704400 -> 1675296704688
	1675276788016 [label="blocks.4.0.conv_pw.weight
 (768, 128, 1, 1)" fillcolor=lightblue]
	1675276788016 -> 1675296704400
	1675296704400 [label=AccumulateGrad]
	1675296704784 -> 1675296704880
	1675276788096 [label="blocks.4.0.bn1.weight
 (768)" fillcolor=lightblue]
	1675276788096 -> 1675296704784
	1675296704784 [label=AccumulateGrad]
	1675296705072 -> 1675296704880
	1675276788176 [label="blocks.4.0.bn1.bias
 (768)" fillcolor=lightblue]
	1675276788176 -> 1675296705072
	1675296705072 [label=AccumulateGrad]
	1675296705120 -> 1675296705360
	1675276788656 [label="blocks.4.0.conv_dw.weight
 (768, 1, 3, 3)" fillcolor=lightblue]
	1675276788656 -> 1675296705120
	1675296705120 [label=AccumulateGrad]
	1675296705456 -> 1675296705552
	1675276788576 [label="blocks.4.0.bn2.weight
 (768)" fillcolor=lightblue]
	1675276788576 -> 1675296705456
	1675296705456 [label=AccumulateGrad]
	1675296705744 -> 1675296705552
	1675276788736 [label="blocks.4.0.bn2.bias
 (768)" fillcolor=lightblue]
	1675276788736 -> 1675296705744
	1675296705744 [label=AccumulateGrad]
	1675296705936 -> 1675296706128
	1675296705936 [label=SigmoidBackward0]
	1675296705264 -> 1675296705936
	1675296705264 [label=ConvolutionBackward0]
	1675296704592 -> 1675296705264
	1675296704592 [label=SiluBackward0]
	1675296704304 -> 1675296704592
	1675296704304 [label=ConvolutionBackward0]
	1675296703632 -> 1675296704304
	1675296703632 [label=MeanBackward1]
	1675296705840 -> 1675296703632
	1675296702864 -> 1675296704304
	1675276789136 [label="blocks.4.0.se.conv_reduce.weight
 (32, 768, 1, 1)" fillcolor=lightblue]
	1675276789136 -> 1675296702864
	1675296702864 [label=AccumulateGrad]
	1675296703680 -> 1675296704304
	1675276789216 [label="blocks.4.0.se.conv_reduce.bias
 (32)" fillcolor=lightblue]
	1675276789216 -> 1675296703680
	1675296703680 [label=AccumulateGrad]
	1675296704496 -> 1675296705264
	1675276789376 [label="blocks.4.0.se.conv_expand.weight
 (768, 32, 1, 1)" fillcolor=lightblue]
	1675276789376 -> 1675296704496
	1675296704496 [label=AccumulateGrad]
	1675296705648 -> 1675296705264
	1675276789456 [label="blocks.4.0.se.conv_expand.bias
 (768)" fillcolor=lightblue]
	1675276789456 -> 1675296705648
	1675296705648 [label=AccumulateGrad]
	1675296706080 -> 1675296706320
	1675276789616 [label="blocks.4.0.conv_pwl.weight
 (160, 768, 1, 1)" fillcolor=lightblue]
	1675276789616 -> 1675296706080
	1675296706080 [label=AccumulateGrad]
	1675296706416 -> 1675296741504
	1675276789696 [label="blocks.4.0.bn3.weight
 (160)" fillcolor=lightblue]
	1675276789696 -> 1675296706416
	1675296706416 [label=AccumulateGrad]
	1675296706512 -> 1675296741504
	1675276789776 [label="blocks.4.0.bn3.bias
 (160)" fillcolor=lightblue]
	1675276789776 -> 1675296706512
	1675296706512 [label=AccumulateGrad]
	1675296739440 -> 1675296739728
	1675276790256 [label="blocks.4.1.conv_pw.weight
 (960, 160, 1, 1)" fillcolor=lightblue]
	1675276790256 -> 1675296739440
	1675296739440 [label=AccumulateGrad]
	1675296739824 -> 1675296739920
	1675276790336 [label="blocks.4.1.bn1.weight
 (960)" fillcolor=lightblue]
	1675276790336 -> 1675296739824
	1675296739824 [label=AccumulateGrad]
	1675296740016 -> 1675296739920
	1675276790416 [label="blocks.4.1.bn1.bias
 (960)" fillcolor=lightblue]
	1675276790416 -> 1675296740016
	1675296740016 [label=AccumulateGrad]
	1675296740112 -> 1675296740256
	1675276790896 [label="blocks.4.1.conv_dw.weight
 (960, 1, 3, 3)" fillcolor=lightblue]
	1675276790896 -> 1675296740112
	1675296740112 [label=AccumulateGrad]
	1675296740400 -> 1675296740496
	1675276790816 [label="blocks.4.1.bn2.weight
 (960)" fillcolor=lightblue]
	1675276790816 -> 1675296740400
	1675296740400 [label=AccumulateGrad]
	1675296740544 -> 1675296740496
	1675276790976 [label="blocks.4.1.bn2.bias
 (960)" fillcolor=lightblue]
	1675276790976 -> 1675296740544
	1675296740544 [label=AccumulateGrad]
	1675296740784 -> 1675296740976
	1675296740784 [label=SigmoidBackward0]
	1675296740208 -> 1675296740784
	1675296740208 [label=ConvolutionBackward0]
	1675296739632 -> 1675296740208
	1675296739632 [label=SiluBackward0]
	1675296706176 -> 1675296739632
	1675296706176 [label=ConvolutionBackward0]
	1675296706032 -> 1675296706176
	1675296706032 [label=MeanBackward1]
	1675296740688 -> 1675296706032
	1675296705216 -> 1675296706176
	1675276791376 [label="blocks.4.1.se.conv_reduce.weight
 (40, 960, 1, 1)" fillcolor=lightblue]
	1675276791376 -> 1675296705216
	1675296705216 [label=AccumulateGrad]
	1675296704976 -> 1675296706176
	1675276791456 [label="blocks.4.1.se.conv_reduce.bias
 (40)" fillcolor=lightblue]
	1675276791456 -> 1675296704976
	1675296704976 [label=AccumulateGrad]
	1675296739536 -> 1675296740208
	1675276791616 [label="blocks.4.1.se.conv_expand.weight
 (960, 40, 1, 1)" fillcolor=lightblue]
	1675276791616 -> 1675296739536
	1675296739536 [label=AccumulateGrad]
	1675296740592 -> 1675296740208
	1675276791696 [label="blocks.4.1.se.conv_expand.bias
 (960)" fillcolor=lightblue]
	1675276791696 -> 1675296740592
	1675296740592 [label=AccumulateGrad]
	1675296741072 -> 1675296741360
	1675276980368 [label="blocks.4.1.conv_pwl.weight
 (160, 960, 1, 1)" fillcolor=lightblue]
	1675276980368 -> 1675296741072
	1675296741072 [label=AccumulateGrad]
	1675296741456 -> 1675296741552
	1675276980448 [label="blocks.4.1.bn3.weight
 (160)" fillcolor=lightblue]
	1675276980448 -> 1675296741456
	1675296741456 [label=AccumulateGrad]
	1675296741408 -> 1675296741552
	1675276980528 [label="blocks.4.1.bn3.bias
 (160)" fillcolor=lightblue]
	1675276980528 -> 1675296741408
	1675296741408 [label=AccumulateGrad]
	1675296741504 -> 1675296764496
	1675296741648 -> 1675296742032
	1675276981008 [label="blocks.4.2.conv_pw.weight
 (960, 160, 1, 1)" fillcolor=lightblue]
	1675276981008 -> 1675296741648
	1675296741648 [label=AccumulateGrad]
	1675296742128 -> 1675296742224
	1675276981088 [label="blocks.4.2.bn1.weight
 (960)" fillcolor=lightblue]
	1675276981088 -> 1675296742128
	1675296742128 [label=AccumulateGrad]
	1675296742416 -> 1675296742224
	1675276981168 [label="blocks.4.2.bn1.bias
 (960)" fillcolor=lightblue]
	1675276981168 -> 1675296742416
	1675296742416 [label=AccumulateGrad]
	1675296742512 -> 1675296742704
	1675276981648 [label="blocks.4.2.conv_dw.weight
 (960, 1, 3, 3)" fillcolor=lightblue]
	1675276981648 -> 1675296742512
	1675296742512 [label=AccumulateGrad]
	1675296742800 -> 1675296742896
	1675276981568 [label="blocks.4.2.bn2.weight
 (960)" fillcolor=lightblue]
	1675276981568 -> 1675296742800
	1675296742800 [label=AccumulateGrad]
	1675296743088 -> 1675296742896
	1675276981728 [label="blocks.4.2.bn2.bias
 (960)" fillcolor=lightblue]
	1675276981728 -> 1675296743088
	1675296743088 [label=AccumulateGrad]
	1675296743280 -> 1675296764016
	1675296743280 [label=SigmoidBackward0]
	1675296742464 -> 1675296743280
	1675296742464 [label=ConvolutionBackward0]
	1675296741936 -> 1675296742464
	1675296741936 [label=SiluBackward0]
	1675296741840 -> 1675296741936
	1675296741840 [label=ConvolutionBackward0]
	1675296740304 -> 1675296741840
	1675296740304 [label=MeanBackward1]
	1675296743184 -> 1675296740304
	1675296739872 -> 1675296741840
	1675276982128 [label="blocks.4.2.se.conv_reduce.weight
 (40, 960, 1, 1)" fillcolor=lightblue]
	1675276982128 -> 1675296739872
	1675296739872 [label=AccumulateGrad]
	1675296741168 -> 1675296741840
	1675276982208 [label="blocks.4.2.se.conv_reduce.bias
 (40)" fillcolor=lightblue]
	1675276982208 -> 1675296741168
	1675296741168 [label=AccumulateGrad]
	1675296741744 -> 1675296742464
	1675276982368 [label="blocks.4.2.se.conv_expand.weight
 (960, 40, 1, 1)" fillcolor=lightblue]
	1675276982368 -> 1675296741744
	1675296741744 [label=AccumulateGrad]
	1675296742992 -> 1675296742464
	1675276982448 [label="blocks.4.2.se.conv_expand.bias
 (960)" fillcolor=lightblue]
	1675276982448 -> 1675296742992
	1675296742992 [label=AccumulateGrad]
	1675296763968 -> 1675296764208
	1675276982608 [label="blocks.4.2.conv_pwl.weight
 (160, 960, 1, 1)" fillcolor=lightblue]
	1675276982608 -> 1675296763968
	1675296763968 [label=AccumulateGrad]
	1675296764304 -> 1675296764400
	1675276982688 [label="blocks.4.2.bn3.weight
 (160)" fillcolor=lightblue]
	1675276982688 -> 1675296764304
	1675296764304 [label=AccumulateGrad]
	1675296764256 -> 1675296764400
	1675276982768 [label="blocks.4.2.bn3.bias
 (160)" fillcolor=lightblue]
	1675276982768 -> 1675296764256
	1675296764256 [label=AccumulateGrad]
	1675296764496 -> 1675296766896
	1675296764592 -> 1675296764880
	1675276983248 [label="blocks.4.3.conv_pw.weight
 (960, 160, 1, 1)" fillcolor=lightblue]
	1675276983248 -> 1675296764592
	1675296764592 [label=AccumulateGrad]
	1675296764976 -> 1675296765072
	1675276983328 [label="blocks.4.3.bn1.weight
 (960)" fillcolor=lightblue]
	1675276983328 -> 1675296764976
	1675296764976 [label=AccumulateGrad]
	1675296765264 -> 1675296765072
	1675276983408 [label="blocks.4.3.bn1.bias
 (960)" fillcolor=lightblue]
	1675276983408 -> 1675296765264
	1675296765264 [label=AccumulateGrad]
	1675296765456 -> 1675296765504
	1675276983888 [label="blocks.4.3.conv_dw.weight
 (960, 1, 3, 3)" fillcolor=lightblue]
	1675276983888 -> 1675296765456
	1675296765456 [label=AccumulateGrad]
	1675296765648 -> 1675296765744
	1675276983808 [label="blocks.4.3.bn2.weight
 (960)" fillcolor=lightblue]
	1675276983808 -> 1675296765648
	1675296765648 [label=AccumulateGrad]
	1675296765936 -> 1675296765744
	1675276983968 [label="blocks.4.3.bn2.bias
 (960)" fillcolor=lightblue]
	1675276983968 -> 1675296765936
	1675296765936 [label=AccumulateGrad]
	1675296766128 -> 1675296766320
	1675296766128 [label=SigmoidBackward0]
	1675296765408 -> 1675296766128
	1675296765408 [label=ConvolutionBackward0]
	1675296764784 -> 1675296765408
	1675296764784 [label=SiluBackward0]
	1675296764688 -> 1675296764784
	1675296764688 [label=ConvolutionBackward0]
	1675296764112 -> 1675296764688
	1675296764112 [label=MeanBackward1]
	1675296766032 -> 1675296764112
	1675296742608 -> 1675296764688
	1675277172880 [label="blocks.4.3.se.conv_reduce.weight
 (40, 960, 1, 1)" fillcolor=lightblue]
	1675277172880 -> 1675296742608
	1675296742608 [label=AccumulateGrad]
	1675296742320 -> 1675296764688
	1675277172960 [label="blocks.4.3.se.conv_reduce.bias
 (40)" fillcolor=lightblue]
	1675277172960 -> 1675296742320
	1675296742320 [label=AccumulateGrad]
	1675296764544 -> 1675296765408
	1675277173120 [label="blocks.4.3.se.conv_expand.weight
 (960, 40, 1, 1)" fillcolor=lightblue]
	1675277173120 -> 1675296764544
	1675296764544 [label=AccumulateGrad]
	1675296765840 -> 1675296765408
	1675277173200 [label="blocks.4.3.se.conv_expand.bias
 (960)" fillcolor=lightblue]
	1675277173200 -> 1675296765840
	1675296765840 [label=AccumulateGrad]
	1675296766416 -> 1675296766464
	1675277173360 [label="blocks.4.3.conv_pwl.weight
 (160, 960, 1, 1)" fillcolor=lightblue]
	1675277173360 -> 1675296766416
	1675296766416 [label=AccumulateGrad]
	1675296766608 -> 1675296766800
	1675277173440 [label="blocks.4.3.bn3.weight
 (160)" fillcolor=lightblue]
	1675277173440 -> 1675296766608
	1675296766608 [label=AccumulateGrad]
	1675296766704 -> 1675296766800
	1675277173520 [label="blocks.4.3.bn3.bias
 (160)" fillcolor=lightblue]
	1675277173520 -> 1675296766704
	1675296766704 [label=AccumulateGrad]
	1675296766896 -> 1675296802032
	1675296766992 -> 1675296767376
	1675277174000 [label="blocks.4.4.conv_pw.weight
 (960, 160, 1, 1)" fillcolor=lightblue]
	1675277174000 -> 1675296766992
	1675296766992 [label=AccumulateGrad]
	1675296767328 -> 1675296767472
	1675277174080 [label="blocks.4.4.bn1.weight
 (960)" fillcolor=lightblue]
	1675277174080 -> 1675296767328
	1675296767328 [label=AccumulateGrad]
	1675296767568 -> 1675296767472
	1675277174160 [label="blocks.4.4.bn1.bias
 (960)" fillcolor=lightblue]
	1675277174160 -> 1675296767568
	1675296767568 [label=AccumulateGrad]
	1675296767760 -> 1675296800880
	1675277174640 [label="blocks.4.4.conv_dw.weight
 (960, 1, 3, 3)" fillcolor=lightblue]
	1675277174640 -> 1675296767760
	1675296767760 [label=AccumulateGrad]
	1675296800832 -> 1675296800976
	1675277174560 [label="blocks.4.4.bn2.weight
 (960)" fillcolor=lightblue]
	1675277174560 -> 1675296800832
	1675296800832 [label=AccumulateGrad]
	1675296801072 -> 1675296800976
	1675277174720 [label="blocks.4.4.bn2.bias
 (960)" fillcolor=lightblue]
	1675277174720 -> 1675296801072
	1675296801072 [label=AccumulateGrad]
	1675296801264 -> 1675296801456
	1675296801264 [label=SigmoidBackward0]
	1675296800928 -> 1675296801264
	1675296800928 [label=ConvolutionBackward0]
	1675296767280 -> 1675296800928
	1675296767280 [label=SiluBackward0]
	1675296767184 -> 1675296767280
	1675296767184 [label=ConvolutionBackward0]
	1675296765552 -> 1675296767184
	1675296765552 [label=MeanBackward1]
	1675296801168 -> 1675296765552
	1675296765168 -> 1675296767184
	1675277175120 [label="blocks.4.4.se.conv_reduce.weight
 (40, 960, 1, 1)" fillcolor=lightblue]
	1675277175120 -> 1675296765168
	1675296765168 [label=AccumulateGrad]
	1675296766368 -> 1675296767184
	1675277175200 [label="blocks.4.4.se.conv_reduce.bias
 (40)" fillcolor=lightblue]
	1675277175200 -> 1675296766368
	1675296766368 [label=AccumulateGrad]
	1675296767088 -> 1675296800928
	1675277175360 [label="blocks.4.4.se.conv_expand.weight
 (960, 40, 1, 1)" fillcolor=lightblue]
	1675277175360 -> 1675296767088
	1675296767088 [label=AccumulateGrad]
	1675296767856 -> 1675296800928
	1675277175440 [label="blocks.4.4.se.conv_expand.bias
 (960)" fillcolor=lightblue]
	1675277175440 -> 1675296767856
	1675296767856 [label=AccumulateGrad]
	1675296801552 -> 1675296801696
	1675277175600 [label="blocks.4.4.conv_pwl.weight
 (160, 960, 1, 1)" fillcolor=lightblue]
	1675277175600 -> 1675296801552
	1675296801552 [label=AccumulateGrad]
	1675296801840 -> 1675296801936
	1675277175680 [label="blocks.4.4.bn3.weight
 (160)" fillcolor=lightblue]
	1675277175680 -> 1675296801840
	1675296801840 [label=AccumulateGrad]
	1675296801792 -> 1675296801936
	1675277175760 [label="blocks.4.4.bn3.bias
 (160)" fillcolor=lightblue]
	1675277175760 -> 1675296801792
	1675296801792 [label=AccumulateGrad]
	1675296802032 -> 1675296804288
	1675296802128 -> 1675296802512
	1675277176240 [label="blocks.4.5.conv_pw.weight
 (960, 160, 1, 1)" fillcolor=lightblue]
	1675277176240 -> 1675296802128
	1675296802128 [label=AccumulateGrad]
	1675296802608 -> 1675296802704
	1675277176320 [label="blocks.4.5.bn1.weight
 (960)" fillcolor=lightblue]
	1675277176320 -> 1675296802608
	1675296802608 [label=AccumulateGrad]
	1675296802800 -> 1675296802704
	1675277176400 [label="blocks.4.5.bn1.bias
 (960)" fillcolor=lightblue]
	1675277176400 -> 1675296802800
	1675296802800 [label=AccumulateGrad]
	1675296802896 -> 1675296803184
	1675277349008 [label="blocks.4.5.conv_dw.weight
 (960, 1, 3, 3)" fillcolor=lightblue]
	1675277349008 -> 1675296802896
	1675296802896 [label=AccumulateGrad]
	1675296803280 -> 1675296803376
	1675277348928 [label="blocks.4.5.bn2.weight
 (960)" fillcolor=lightblue]
	1675277348928 -> 1675296803280
	1675296803280 [label=AccumulateGrad]
	1675296803568 -> 1675296803376
	1675277349088 [label="blocks.4.5.bn2.bias
 (960)" fillcolor=lightblue]
	1675277349088 -> 1675296803568
	1675296803568 [label=AccumulateGrad]
	1675296803616 -> 1675296803712
	1675296803616 [label=SigmoidBackward0]
	1675296802992 -> 1675296803616
	1675296802992 [label=ConvolutionBackward0]
	1675296802416 -> 1675296802992
	1675296802416 [label=SiluBackward0]
	1675296802320 -> 1675296802416
	1675296802320 [label=ConvolutionBackward0]
	1675296801360 -> 1675296802320
	1675296801360 [label=MeanBackward1]
	1675296803664 -> 1675296801360
	1675296801648 -> 1675296802320
	1675277349488 [label="blocks.4.5.se.conv_reduce.weight
 (40, 960, 1, 1)" fillcolor=lightblue]
	1675277349488 -> 1675296801648
	1675296801648 [label=AccumulateGrad]
	1675296767424 -> 1675296802320
	1675277349568 [label="blocks.4.5.se.conv_reduce.bias
 (40)" fillcolor=lightblue]
	1675277349568 -> 1675296767424
	1675296767424 [label=AccumulateGrad]
	1675296802224 -> 1675296802992
	1675277349728 [label="blocks.4.5.se.conv_expand.weight
 (960, 40, 1, 1)" fillcolor=lightblue]
	1675277349728 -> 1675296802224
	1675296802224 [label=AccumulateGrad]
	1675296803472 -> 1675296802992
	1675277349808 [label="blocks.4.5.se.conv_expand.bias
 (960)" fillcolor=lightblue]
	1675277349808 -> 1675296803472
	1675296803472 [label=AccumulateGrad]
	1675296803856 -> 1675296804000
	1675277349968 [label="blocks.4.5.conv_pwl.weight
 (160, 960, 1, 1)" fillcolor=lightblue]
	1675277349968 -> 1675296803856
	1675296803856 [label=AccumulateGrad]
	1675296804144 -> 1675296804336
	1675277350048 [label="blocks.4.5.bn3.weight
 (160)" fillcolor=lightblue]
	1675277350048 -> 1675296804144
	1675296804144 [label=AccumulateGrad]
	1675296804240 -> 1675296804336
	1675277350128 [label="blocks.4.5.bn3.bias
 (160)" fillcolor=lightblue]
	1675277350128 -> 1675296804240
	1675296804240 [label=AccumulateGrad]
	1675296804288 -> 1675296835568
	1675296804432 -> 1675296833648
	1675277350608 [label="blocks.4.6.conv_pw.weight
 (960, 160, 1, 1)" fillcolor=lightblue]
	1675277350608 -> 1675296804432
	1675296804432 [label=AccumulateGrad]
	1675296833936 -> 1675296833744
	1675277350688 [label="blocks.4.6.bn1.weight
 (960)" fillcolor=lightblue]
	1675277350688 -> 1675296833936
	1675296833936 [label=AccumulateGrad]
	1675296804816 -> 1675296833744
	1675277350768 [label="blocks.4.6.bn1.bias
 (960)" fillcolor=lightblue]
	1675277350768 -> 1675296804816
	1675296804816 [label=AccumulateGrad]
	1675296834032 -> 1675296834224
	1675277351248 [label="blocks.4.6.conv_dw.weight
 (960, 1, 3, 3)" fillcolor=lightblue]
	1675277351248 -> 1675296834032
	1675296834032 [label=AccumulateGrad]
	1675296834320 -> 1675296834416
	1675277351168 [label="blocks.4.6.bn2.weight
 (960)" fillcolor=lightblue]
	1675277351168 -> 1675296834320
	1675296834320 [label=AccumulateGrad]
	1675296834608 -> 1675296834416
	1675277351328 [label="blocks.4.6.bn2.bias
 (960)" fillcolor=lightblue]
	1675277351328 -> 1675296834608
	1675296834608 [label=AccumulateGrad]
	1675296834800 -> 1675296834848
	1675296834800 [label=SigmoidBackward0]
	1675296833984 -> 1675296834800
	1675296833984 [label=ConvolutionBackward0]
	1675296833840 -> 1675296833984
	1675296833840 [label=SiluBackward0]
	1675296804624 -> 1675296833840
	1675296804624 [label=ConvolutionBackward0]
	1675296803088 -> 1675296804624
	1675296803088 [label=MeanBackward1]
	1675296834704 -> 1675296803088
	1675296802656 -> 1675296804624
	1675277351728 [label="blocks.4.6.se.conv_reduce.weight
 (40, 960, 1, 1)" fillcolor=lightblue]
	1675277351728 -> 1675296802656
	1675296802656 [label=AccumulateGrad]
	1675296803952 -> 1675296804624
	1675277351808 [label="blocks.4.6.se.conv_reduce.bias
 (40)" fillcolor=lightblue]
	1675277351808 -> 1675296803952
	1675296803952 [label=AccumulateGrad]
	1675296834512 -> 1675296833984
	1675277351968 [label="blocks.4.6.se.conv_expand.weight
 (960, 40, 1, 1)" fillcolor=lightblue]
	1675277351968 -> 1675296834512
	1675296834512 [label=AccumulateGrad]
	1675296804528 -> 1675296833984
	1675277352048 [label="blocks.4.6.se.conv_expand.bias
 (960)" fillcolor=lightblue]
	1675277352048 -> 1675296804528
	1675296804528 [label=AccumulateGrad]
	1675296834992 -> 1675296835184
	1675277352208 [label="blocks.4.6.conv_pwl.weight
 (160, 960, 1, 1)" fillcolor=lightblue]
	1675277352208 -> 1675296834992
	1675296834992 [label=AccumulateGrad]
	1675296835280 -> 1675296835472
	1675277352288 [label="blocks.4.6.bn3.weight
 (160)" fillcolor=lightblue]
	1675277352288 -> 1675296835280
	1675296835280 [label=AccumulateGrad]
	1675296835376 -> 1675296835472
	1675277352368 [label="blocks.4.6.bn3.bias
 (160)" fillcolor=lightblue]
	1675277352368 -> 1675296835376
	1675296835376 [label=AccumulateGrad]
	1675296835568 -> 1675296858416
	1675296835664 -> 1675296835952
	1675277352848 [label="blocks.4.7.conv_pw.weight
 (960, 160, 1, 1)" fillcolor=lightblue]
	1675277352848 -> 1675296835664
	1675296835664 [label=AccumulateGrad]
	1675296835904 -> 1675296836048
	1675277541440 [label="blocks.4.7.bn1.weight
 (960)" fillcolor=lightblue]
	1675277541440 -> 1675296835904
	1675296835904 [label=AccumulateGrad]
	1675296836240 -> 1675296836048
	1675277541520 [label="blocks.4.7.bn1.bias
 (960)" fillcolor=lightblue]
	1675277541520 -> 1675296836240
	1675296836240 [label=AccumulateGrad]
	1675296836336 -> 1675296836480
	1675277542000 [label="blocks.4.7.conv_dw.weight
 (960, 1, 3, 3)" fillcolor=lightblue]
	1675277542000 -> 1675296836336
	1675296836336 [label=AccumulateGrad]
	1675296836624 -> 1675296836720
	1675277541920 [label="blocks.4.7.bn2.weight
 (960)" fillcolor=lightblue]
	1675277541920 -> 1675296836624
	1675296836624 [label=AccumulateGrad]
	1675296836912 -> 1675296836720
	1675277542080 [label="blocks.4.7.bn2.bias
 (960)" fillcolor=lightblue]
	1675277542080 -> 1675296836912
	1675296836912 [label=AccumulateGrad]
	1675296837104 -> 1675296837296
	1675296837104 [label=SigmoidBackward0]
	1675296836432 -> 1675296837104
	1675296836432 [label=ConvolutionBackward0]
	1675296835808 -> 1675296836432
	1675296835808 [label=SiluBackward0]
	1675296835856 -> 1675296835808
	1675296835856 [label=ConvolutionBackward0]
	1675296834896 -> 1675296835856
	1675296834896 [label=MeanBackward1]
	1675296837008 -> 1675296834896
	1675296834128 -> 1675296835856
	1675277542560 [label="blocks.4.7.se.conv_reduce.weight
 (40, 960, 1, 1)" fillcolor=lightblue]
	1675277542560 -> 1675296834128
	1675296834128 [label=AccumulateGrad]
	1675296834944 -> 1675296835856
	1675277542640 [label="blocks.4.7.se.conv_reduce.bias
 (40)" fillcolor=lightblue]
	1675277542640 -> 1675296834944
	1675296834944 [label=AccumulateGrad]
	1675296835760 -> 1675296836432
	1675277542800 [label="blocks.4.7.se.conv_expand.weight
 (960, 40, 1, 1)" fillcolor=lightblue]
	1675277542800 -> 1675296835760
	1675296835760 [label=AccumulateGrad]
	1675296836816 -> 1675296836432
	1675277542880 [label="blocks.4.7.se.conv_expand.bias
 (960)" fillcolor=lightblue]
	1675277542880 -> 1675296836816
	1675296836816 [label=AccumulateGrad]
	1675296837392 -> 1675296858224
	1675277543040 [label="blocks.4.7.conv_pwl.weight
 (160, 960, 1, 1)" fillcolor=lightblue]
	1675277543040 -> 1675296837392
	1675296837392 [label=AccumulateGrad]
	1675296837440 -> 1675296858320
	1675277543120 [label="blocks.4.7.bn3.weight
 (160)" fillcolor=lightblue]
	1675277543120 -> 1675296837440
	1675296837440 [label=AccumulateGrad]
	1675296837584 -> 1675296858320
	1675277543200 [label="blocks.4.7.bn3.bias
 (160)" fillcolor=lightblue]
	1675277543200 -> 1675296837584
	1675296837584 [label=AccumulateGrad]
	1675296858416 -> 1675296860720
	1675296858512 -> 1675296858896
	1675277543680 [label="blocks.4.8.conv_pw.weight
 (960, 160, 1, 1)" fillcolor=lightblue]
	1675277543680 -> 1675296858512
	1675296858512 [label=AccumulateGrad]
	1675296858848 -> 1675296858992
	1675277543760 [label="blocks.4.8.bn1.weight
 (960)" fillcolor=lightblue]
	1675277543760 -> 1675296858848
	1675296858848 [label=AccumulateGrad]
	1675296859088 -> 1675296858992
	1675277543840 [label="blocks.4.8.bn1.bias
 (960)" fillcolor=lightblue]
	1675277543840 -> 1675296859088
	1675296859088 [label=AccumulateGrad]
	1675296859280 -> 1675296859568
	1675277544320 [label="blocks.4.8.conv_dw.weight
 (960, 1, 3, 3)" fillcolor=lightblue]
	1675277544320 -> 1675296859280
	1675296859280 [label=AccumulateGrad]
	1675296859664 -> 1675296859760
	1675277544240 [label="blocks.4.8.bn2.weight
 (960)" fillcolor=lightblue]
	1675277544240 -> 1675296859664
	1675296859664 [label=AccumulateGrad]
	1675296859808 -> 1675296859760
	1675277544400 [label="blocks.4.8.bn2.bias
 (960)" fillcolor=lightblue]
	1675277544400 -> 1675296859808
	1675296859808 [label=AccumulateGrad]
	1675296859904 -> 1675296860144
	1675296859904 [label=SigmoidBackward0]
	1675296859376 -> 1675296859904
	1675296859376 [label=ConvolutionBackward0]
	1675296858800 -> 1675296859376
	1675296858800 [label=SiluBackward0]
	1675296858704 -> 1675296858800
	1675296858704 [label=ConvolutionBackward0]
	1675296836528 -> 1675296858704
	1675296836528 [label=MeanBackward1]
	1675296859952 -> 1675296836528
	1675296836144 -> 1675296858704
	1675277544800 [label="blocks.4.8.se.conv_reduce.weight
 (40, 960, 1, 1)" fillcolor=lightblue]
	1675277544800 -> 1675296836144
	1675296836144 [label=AccumulateGrad]
	1675296837344 -> 1675296858704
	1675277544880 [label="blocks.4.8.se.conv_reduce.bias
 (40)" fillcolor=lightblue]
	1675277544880 -> 1675296837344
	1675296837344 [label=AccumulateGrad]
	1675296858608 -> 1675296859376
	1675277545040 [label="blocks.4.8.se.conv_expand.weight
 (960, 40, 1, 1)" fillcolor=lightblue]
	1675277545040 -> 1675296858608
	1675296858608 [label=AccumulateGrad]
	1675296859856 -> 1675296859376
	1675277545120 [label="blocks.4.8.se.conv_expand.bias
 (960)" fillcolor=lightblue]
	1675277545120 -> 1675296859856
	1675296859856 [label=AccumulateGrad]
	1675296860240 -> 1675296860432
	1675277545280 [label="blocks.4.8.conv_pwl.weight
 (160, 960, 1, 1)" fillcolor=lightblue]
	1675277545280 -> 1675296860240
	1675296860240 [label=AccumulateGrad]
	1675296860528 -> 1675296860624
	1675277545360 [label="blocks.4.8.bn3.weight
 (160)" fillcolor=lightblue]
	1675277545360 -> 1675296860528
	1675296860528 [label=AccumulateGrad]
	1675296860480 -> 1675296860624
	1675277721664 [label="blocks.4.8.bn3.bias
 (160)" fillcolor=lightblue]
	1675277721664 -> 1675296860480
	1675296860480 [label=AccumulateGrad]
	1675296860720 -> 1675296860912
	1675296861008 -> 1675296861296
	1675277722144 [label="blocks.5.0.conv_pw.weight
 (960, 160, 1, 1)" fillcolor=lightblue]
	1675277722144 -> 1675296861008
	1675296861008 [label=AccumulateGrad]
	1675296861392 -> 1675296861344
	1675277722224 [label="blocks.5.0.bn1.weight
 (960)" fillcolor=lightblue]
	1675277722224 -> 1675296861392
	1675296861392 [label=AccumulateGrad]
	1675296861680 -> 1675296861344
	1675277722304 [label="blocks.5.0.bn1.bias
 (960)" fillcolor=lightblue]
	1675277722304 -> 1675296861680
	1675296861680 [label=AccumulateGrad]
	1675296861872 -> 1675296890992
	1675277722784 [label="blocks.5.0.conv_dw.weight
 (960, 1, 3, 3)" fillcolor=lightblue]
	1675277722784 -> 1675296861872
	1675296861872 [label=AccumulateGrad]
	1675296891184 -> 1675296891088
	1675277722704 [label="blocks.5.0.bn2.weight
 (960)" fillcolor=lightblue]
	1675277722704 -> 1675296891184
	1675296891184 [label=AccumulateGrad]
	1675296862160 -> 1675296891088
	1675277722864 [label="blocks.5.0.bn2.bias
 (960)" fillcolor=lightblue]
	1675277722864 -> 1675296862160
	1675296862160 [label=AccumulateGrad]
	1675296891280 -> 1675296891472
	1675296891280 [label=SigmoidBackward0]
	1675296891040 -> 1675296891280
	1675296891040 [label=ConvolutionBackward0]
	1675296861488 -> 1675296891040
	1675296861488 [label=SiluBackward0]
	1675296860336 -> 1675296861488
	1675296860336 [label=ConvolutionBackward0]
	1675296858944 -> 1675296860336
	1675296858944 [label=MeanBackward1]
	1675296891136 -> 1675296858944
	1675296860816 -> 1675296860336
	1675277723264 [label="blocks.5.0.se.conv_reduce.weight
 (40, 960, 1, 1)" fillcolor=lightblue]
	1675277723264 -> 1675296860816
	1675296860816 [label=AccumulateGrad]
	1675296861104 -> 1675296860336
	1675277723344 [label="blocks.5.0.se.conv_reduce.bias
 (40)" fillcolor=lightblue]
	1675277723344 -> 1675296861104
	1675296861104 [label=AccumulateGrad]
	1675296861200 -> 1675296891040
	1675277723504 [label="blocks.5.0.se.conv_expand.weight
 (960, 40, 1, 1)" fillcolor=lightblue]
	1675277723504 -> 1675296861200
	1675296861200 [label=AccumulateGrad]
	1675296862064 -> 1675296891040
	1675277723584 [label="blocks.5.0.se.conv_expand.bias
 (960)" fillcolor=lightblue]
	1675277723584 -> 1675296862064
	1675296862064 [label=AccumulateGrad]
	1675296891568 -> 1675296891856
	1675277723744 [label="blocks.5.0.conv_pwl.weight
 (256, 960, 1, 1)" fillcolor=lightblue]
	1675277723744 -> 1675296891568
	1675296891568 [label=AccumulateGrad]
	1675296891952 -> 1675296894256
	1675277723824 [label="blocks.5.0.bn3.weight
 (256)" fillcolor=lightblue]
	1675277723824 -> 1675296891952
	1675296891952 [label=AccumulateGrad]
	1675296892144 -> 1675296894256
	1675277723904 [label="blocks.5.0.bn3.bias
 (256)" fillcolor=lightblue]
	1675277723904 -> 1675296892144
	1675296892144 [label=AccumulateGrad]
	1675296892048 -> 1675296892240
	1675277724384 [label="blocks.5.1.conv_pw.weight
 (1536, 256, 1, 1)" fillcolor=lightblue]
	1675277724384 -> 1675296892048
	1675296892048 [label=AccumulateGrad]
	1675296892336 -> 1675296892432
	1675277724464 [label="blocks.5.1.bn1.weight
 (1536)" fillcolor=lightblue]
	1675277724464 -> 1675296892336
	1675296892336 [label=AccumulateGrad]
	1675296892528 -> 1675296892432
	1675277724544 [label="blocks.5.1.bn1.bias
 (1536)" fillcolor=lightblue]
	1675277724544 -> 1675296892528
	1675296892528 [label=AccumulateGrad]
	1675296892720 -> 1675296892912
	1675277725024 [label="blocks.5.1.conv_dw.weight
 (1536, 1, 3, 3)" fillcolor=lightblue]
	1675277725024 -> 1675296892720
	1675296892720 [label=AccumulateGrad]
	1675296893008 -> 1675296893104
	1675277724944 [label="blocks.5.1.bn2.weight
 (1536)" fillcolor=lightblue]
	1675277724944 -> 1675296893008
	1675296893008 [label=AccumulateGrad]
	1675296893296 -> 1675296893104
	1675277725104 [label="blocks.5.1.bn2.bias
 (1536)" fillcolor=lightblue]
	1675277725104 -> 1675296893296
	1675296893296 [label=AccumulateGrad]
	1675296893488 -> 1675296893536
	1675296893488 [label=SigmoidBackward0]
	1675296892672 -> 1675296893488
	1675296892672 [label=ConvolutionBackward0]
	1675296892096 -> 1675296892672
	1675296892096 [label=SiluBackward0]
	1675296891760 -> 1675296892096
	1675296891760 [label=ConvolutionBackward0]
	1675296891376 -> 1675296891760
	1675296891376 [label=MeanBackward1]
	1675296893392 -> 1675296891376
	1675296861968 -> 1675296891760
	1675277725504 [label="blocks.5.1.se.conv_reduce.weight
 (64, 1536, 1, 1)" fillcolor=lightblue]
	1675277725504 -> 1675296861968
	1675296861968 [label=AccumulateGrad]
	1675296861584 -> 1675296891760
	1675277725584 [label="blocks.5.1.se.conv_reduce.bias
 (64)" fillcolor=lightblue]
	1675277725584 -> 1675296861584
	1675296861584 [label=AccumulateGrad]
	1675296892000 -> 1675296892672
	1675277926544 [label="blocks.5.1.se.conv_expand.weight
 (1536, 64, 1, 1)" fillcolor=lightblue]
	1675277926544 -> 1675296892000
	1675296892000 [label=AccumulateGrad]
	1675296893200 -> 1675296892672
	1675277926624 [label="blocks.5.1.se.conv_expand.bias
 (1536)" fillcolor=lightblue]
	1675277926624 -> 1675296893200
	1675296893200 [label=AccumulateGrad]
	1675296893680 -> 1675296893872
	1675277926784 [label="blocks.5.1.conv_pwl.weight
 (256, 1536, 1, 1)" fillcolor=lightblue]
	1675277926784 -> 1675296893680
	1675296893680 [label=AccumulateGrad]
	1675296893968 -> 1675296894160
	1675277926864 [label="blocks.5.1.bn3.weight
 (256)" fillcolor=lightblue]
	1675277926864 -> 1675296893968
	1675296893968 [label=AccumulateGrad]
	1675296894064 -> 1675296894160
	1675277926944 [label="blocks.5.1.bn3.bias
 (256)" fillcolor=lightblue]
	1675277926944 -> 1675296894064
	1675296894064 [label=AccumulateGrad]
	1675296894256 -> 1675296925296
	1675296894352 -> 1675296894640
	1675277927424 [label="blocks.5.2.conv_pw.weight
 (1536, 256, 1, 1)" fillcolor=lightblue]
	1675277927424 -> 1675296894352
	1675296894352 [label=AccumulateGrad]
	1675296894592 -> 1675296894736
	1675277927504 [label="blocks.5.2.bn1.weight
 (1536)" fillcolor=lightblue]
	1675277927504 -> 1675296894592
	1675296894592 [label=AccumulateGrad]
	1675296894928 -> 1675296894736
	1675277927584 [label="blocks.5.2.bn1.bias
 (1536)" fillcolor=lightblue]
	1675277927584 -> 1675296894928
	1675296894928 [label=AccumulateGrad]
	1675296923856 -> 1675296924144
	1675277928064 [label="blocks.5.2.conv_dw.weight
 (1536, 1, 3, 3)" fillcolor=lightblue]
	1675277928064 -> 1675296923856
	1675296923856 [label=AccumulateGrad]
	1675296924240 -> 1675296924192
	1675277927984 [label="blocks.5.2.bn2.weight
 (1536)" fillcolor=lightblue]
	1675277927984 -> 1675296924240
	1675296924240 [label=AccumulateGrad]
	1675296924288 -> 1675296924192
	1675277928144 [label="blocks.5.2.bn2.bias
 (1536)" fillcolor=lightblue]
	1675277928144 -> 1675296924288
	1675296924288 [label=AccumulateGrad]
	1675296924528 -> 1675296924576
	1675296924528 [label=SigmoidBackward0]
	1675296923952 -> 1675296924528
	1675296923952 [label=ConvolutionBackward0]
	1675296924336 -> 1675296923952
	1675296924336 [label=SiluBackward0]
	1675296894544 -> 1675296924336
	1675296894544 [label=ConvolutionBackward0]
	1675296892816 -> 1675296894544
	1675296892816 [label=MeanBackward1]
	1675296924432 -> 1675296892816
	1675296892384 -> 1675296894544
	1675277928544 [label="blocks.5.2.se.conv_reduce.weight
 (64, 1536, 1, 1)" fillcolor=lightblue]
	1675277928544 -> 1675296892384
	1675296892384 [label=AccumulateGrad]
	1675296893632 -> 1675296894544
	1675277928624 [label="blocks.5.2.se.conv_reduce.bias
 (64)" fillcolor=lightblue]
	1675277928624 -> 1675296893632
	1675296893632 [label=AccumulateGrad]
	1675296894496 -> 1675296923952
	1675277928784 [label="blocks.5.2.se.conv_expand.weight
 (1536, 64, 1, 1)" fillcolor=lightblue]
	1675277928784 -> 1675296894496
	1675296894496 [label=AccumulateGrad]
	1675296894448 -> 1675296923952
	1675277928864 [label="blocks.5.2.se.conv_expand.bias
 (1536)" fillcolor=lightblue]
	1675277928864 -> 1675296894448
	1675296894448 [label=AccumulateGrad]
	1675296924720 -> 1675296924864
	1675277929024 [label="blocks.5.2.conv_pwl.weight
 (256, 1536, 1, 1)" fillcolor=lightblue]
	1675277929024 -> 1675296924720
	1675296924720 [label=AccumulateGrad]
	1675296925008 -> 1675296925200
	1675277929104 [label="blocks.5.2.bn3.weight
 (256)" fillcolor=lightblue]
	1675277929104 -> 1675296925008
	1675296925008 [label=AccumulateGrad]
	1675296925104 -> 1675296925200
	1675277929184 [label="blocks.5.2.bn3.bias
 (256)" fillcolor=lightblue]
	1675277929184 -> 1675296925104
	1675296925104 [label=AccumulateGrad]
	1675296925296 -> 1675296948384
	1675296925392 -> 1675296925776
	1675277929664 [label="blocks.5.3.conv_pw.weight
 (1536, 256, 1, 1)" fillcolor=lightblue]
	1675277929664 -> 1675296925392
	1675296925392 [label=AccumulateGrad]
	1675296925728 -> 1675296925872
	1675277929744 [label="blocks.5.3.bn1.weight
 (1536)" fillcolor=lightblue]
	1675277929744 -> 1675296925728
	1675296925728 [label=AccumulateGrad]
	1675296925968 -> 1675296925872
	1675277929824 [label="blocks.5.3.bn1.bias
 (1536)" fillcolor=lightblue]
	1675277929824 -> 1675296925968
	1675296925968 [label=AccumulateGrad]
	1675296926160 -> 1675296926448
	1675277930304 [label="blocks.5.3.conv_dw.weight
 (1536, 1, 3, 3)" fillcolor=lightblue]
	1675277930304 -> 1675296926160
	1675296926160 [label=AccumulateGrad]
	1675296926544 -> 1675296926640
	1675277930224 [label="blocks.5.3.bn2.weight
 (1536)" fillcolor=lightblue]
	1675277930224 -> 1675296926544
	1675296926544 [label=AccumulateGrad]
	1675296926688 -> 1675296926640
	1675277930384 [label="blocks.5.3.bn2.bias
 (1536)" fillcolor=lightblue]
	1675277930384 -> 1675296926688
	1675296926688 [label=AccumulateGrad]
	1675296926784 -> 1675296927024
	1675296926784 [label=SigmoidBackward0]
	1675296926256 -> 1675296926784
	1675296926256 [label=ConvolutionBackward0]
	1675296925680 -> 1675296926256
	1675296925680 [label=SiluBackward0]
	1675296925584 -> 1675296925680
	1675296925584 [label=ConvolutionBackward0]
	1675296924624 -> 1675296925584
	1675296924624 [label=MeanBackward1]
	1675296926832 -> 1675296924624
	1675296924048 -> 1675296925584
	1675278107008 [label="blocks.5.3.se.conv_reduce.weight
 (64, 1536, 1, 1)" fillcolor=lightblue]
	1675278107008 -> 1675296924048
	1675296924048 [label=AccumulateGrad]
	1675296924816 -> 1675296925584
	1675278107088 [label="blocks.5.3.se.conv_reduce.bias
 (64)" fillcolor=lightblue]
	1675278107088 -> 1675296924816
	1675296924816 [label=AccumulateGrad]
	1675296925488 -> 1675296926256
	1675278107248 [label="blocks.5.3.se.conv_expand.weight
 (1536, 64, 1, 1)" fillcolor=lightblue]
	1675278107248 -> 1675296925488
	1675296925488 [label=AccumulateGrad]
	1675296926736 -> 1675296926256
	1675278107328 [label="blocks.5.3.se.conv_expand.bias
 (1536)" fillcolor=lightblue]
	1675278107328 -> 1675296926736
	1675296926736 [label=AccumulateGrad]
	1675296927120 -> 1675296927408
	1675278107488 [label="blocks.5.3.conv_pwl.weight
 (256, 1536, 1, 1)" fillcolor=lightblue]
	1675278107488 -> 1675296927120
	1675296927120 [label=AccumulateGrad]
	1675296927504 -> 1675296948336
	1675278107568 [label="blocks.5.3.bn3.weight
 (256)" fillcolor=lightblue]
	1675278107568 -> 1675296927504
	1675296927504 [label=AccumulateGrad]
	1675296927600 -> 1675296948336
	1675278107648 [label="blocks.5.3.bn3.bias
 (256)" fillcolor=lightblue]
	1675278107648 -> 1675296927600
	1675296927600 [label=AccumulateGrad]
	1675296948384 -> 1675296950640
	1675296948288 -> 1675296948624
	1675278108128 [label="blocks.5.4.conv_pw.weight
 (1536, 256, 1, 1)" fillcolor=lightblue]
	1675278108128 -> 1675296948288
	1675296948288 [label=AccumulateGrad]
	1675296948576 -> 1675296948720
	1675278108208 [label="blocks.5.4.bn1.weight
 (1536)" fillcolor=lightblue]
	1675278108208 -> 1675296948576
	1675296948576 [label=AccumulateGrad]
	1675296948912 -> 1675296948720
	1675278108288 [label="blocks.5.4.bn1.bias
 (1536)" fillcolor=lightblue]
	1675278108288 -> 1675296948912
	1675296948912 [label=AccumulateGrad]
	1675296949008 -> 1675296949296
	1675278108768 [label="blocks.5.4.conv_dw.weight
 (1536, 1, 3, 3)" fillcolor=lightblue]
	1675278108768 -> 1675296949008
	1675296949008 [label=AccumulateGrad]
	1675296949392 -> 1675296949488
	1675278108688 [label="blocks.5.4.bn2.weight
 (1536)" fillcolor=lightblue]
	1675278108688 -> 1675296949392
	1675296949392 [label=AccumulateGrad]
	1675296949680 -> 1675296949488
	1675278108848 [label="blocks.5.4.bn2.bias
 (1536)" fillcolor=lightblue]
	1675278108848 -> 1675296949680
	1675296949680 [label=AccumulateGrad]
	1675296949728 -> 1675296949824
	1675296949728 [label=SigmoidBackward0]
	1675296949104 -> 1675296949728
	1675296949104 [label=ConvolutionBackward0]
	1675296948528 -> 1675296949104
	1675296948528 [label=SiluBackward0]
	1675296927696 -> 1675296948528
	1675296927696 [label=ConvolutionBackward0]
	1675296926352 -> 1675296927696
	1675296926352 [label=MeanBackward1]
	1675296949776 -> 1675296926352
	1675296925824 -> 1675296927696
	1675278109248 [label="blocks.5.4.se.conv_reduce.weight
 (64, 1536, 1, 1)" fillcolor=lightblue]
	1675278109248 -> 1675296925824
	1675296925824 [label=AccumulateGrad]
	1675296927216 -> 1675296927696
	1675278109328 [label="blocks.5.4.se.conv_reduce.bias
 (64)" fillcolor=lightblue]
	1675278109328 -> 1675296927216
	1675296927216 [label=AccumulateGrad]
	1675296948432 -> 1675296949104
	1675278109488 [label="blocks.5.4.se.conv_expand.weight
 (1536, 64, 1, 1)" fillcolor=lightblue]
	1675278109488 -> 1675296948432
	1675296948432 [label=AccumulateGrad]
	1675296949584 -> 1675296949104
	1675278109568 [label="blocks.5.4.se.conv_expand.bias
 (1536)" fillcolor=lightblue]
	1675278109568 -> 1675296949584
	1675296949584 [label=AccumulateGrad]
	1675296949968 -> 1675296950256
	1675278109728 [label="blocks.5.4.conv_pwl.weight
 (256, 1536, 1, 1)" fillcolor=lightblue]
	1675278109728 -> 1675296949968
	1675296949968 [label=AccumulateGrad]
	1675296950352 -> 1675296950544
	1675278109808 [label="blocks.5.4.bn3.weight
 (256)" fillcolor=lightblue]
	1675278109808 -> 1675296950352
	1675296950352 [label=AccumulateGrad]
	1675296950448 -> 1675296950544
	1675278109888 [label="blocks.5.4.bn3.bias
 (256)" fillcolor=lightblue]
	1675278109888 -> 1675296950448
	1675296950448 [label=AccumulateGrad]
	1675296950640 -> 1675296981680
	1675296950736 -> 1675296950928
	1675278110368 [label="blocks.5.5.conv_pw.weight
 (1536, 256, 1, 1)" fillcolor=lightblue]
	1675278110368 -> 1675296950736
	1675296950736 [label=AccumulateGrad]
	1675296951024 -> 1675296951120
	1675278110448 [label="blocks.5.5.bn1.weight
 (1536)" fillcolor=lightblue]
	1675278110448 -> 1675296951024
	1675296951024 [label=AccumulateGrad]
	1675296951312 -> 1675296951120
	1675278110528 [label="blocks.5.5.bn1.bias
 (1536)" fillcolor=lightblue]
	1675278110528 -> 1675296951312
	1675296951312 [label=AccumulateGrad]
	1675296951504 -> 1675296951648
	1675278291328 [label="blocks.5.5.conv_dw.weight
 (1536, 1, 3, 3)" fillcolor=lightblue]
	1675278291328 -> 1675296951504
	1675296951504 [label=AccumulateGrad]
	1675296951792 -> 1675296951744
	1675278291248 [label="blocks.5.5.bn2.weight
 (1536)" fillcolor=lightblue]
	1675278291248 -> 1675296951792
	1675296951792 [label=AccumulateGrad]
	1675296951984 -> 1675296951744
	1675278291408 [label="blocks.5.5.bn2.bias
 (1536)" fillcolor=lightblue]
	1675278291408 -> 1675296951984
	1675296951984 [label=AccumulateGrad]
	1675296952032 -> 1675296981104
	1675296952032 [label=SigmoidBackward0]
	1675296951600 -> 1675296952032
	1675296951600 [label=ConvolutionBackward0]
	1675296950784 -> 1675296951600
	1675296950784 [label=SiluBackward0]
	1675296950832 -> 1675296950784
	1675296950832 [label=ConvolutionBackward0]
	1675296949200 -> 1675296950832
	1675296949200 [label=MeanBackward1]
	1675296952080 -> 1675296949200
	1675296948816 -> 1675296950832
	1675278291808 [label="blocks.5.5.se.conv_reduce.weight
 (64, 1536, 1, 1)" fillcolor=lightblue]
	1675278291808 -> 1675296948816
	1675296948816 [label=AccumulateGrad]
	1675296950064 -> 1675296950832
	1675278291888 [label="blocks.5.5.se.conv_reduce.bias
 (64)" fillcolor=lightblue]
	1675278291888 -> 1675296950064
	1675296950064 [label=AccumulateGrad]
	1675296950688 -> 1675296951600
	1675278292048 [label="blocks.5.5.se.conv_expand.weight
 (1536, 64, 1, 1)" fillcolor=lightblue]
	1675278292048 -> 1675296950688
	1675296950688 [label=AccumulateGrad]
	1675296951888 -> 1675296951600
	1675278292128 [label="blocks.5.5.se.conv_expand.bias
 (1536)" fillcolor=lightblue]
	1675278292128 -> 1675296951888
	1675296951888 [label=AccumulateGrad]
	1675296981056 -> 1675296981296
	1675278292288 [label="blocks.5.5.conv_pwl.weight
 (256, 1536, 1, 1)" fillcolor=lightblue]
	1675278292288 -> 1675296981056
	1675296981056 [label=AccumulateGrad]
	1675296981392 -> 1675296981584
	1675278292368 [label="blocks.5.5.bn3.weight
 (256)" fillcolor=lightblue]
	1675278292368 -> 1675296981392
	1675296981392 [label=AccumulateGrad]
	1675296981488 -> 1675296981584
	1675278292448 [label="blocks.5.5.bn3.bias
 (256)" fillcolor=lightblue]
	1675278292448 -> 1675296981488
	1675296981488 [label=AccumulateGrad]
	1675296981680 -> 1675296983936
	1675296981776 -> 1675296982064
	1675278292928 [label="blocks.5.6.conv_pw.weight
 (1536, 256, 1, 1)" fillcolor=lightblue]
	1675278292928 -> 1675296981776
	1675296981776 [label=AccumulateGrad]
	1675296982016 -> 1675296982160
	1675278293008 [label="blocks.5.6.bn1.weight
 (1536)" fillcolor=lightblue]
	1675278293008 -> 1675296982016
	1675296982016 [label=AccumulateGrad]
	1675296982352 -> 1675296982160
	1675278293088 [label="blocks.5.6.bn1.bias
 (1536)" fillcolor=lightblue]
	1675278293088 -> 1675296982352
	1675296982352 [label=AccumulateGrad]
	1675296982544 -> 1675296982832
	1675278293568 [label="blocks.5.6.conv_dw.weight
 (1536, 1, 3, 3)" fillcolor=lightblue]
	1675278293568 -> 1675296982544
	1675296982544 [label=AccumulateGrad]
	1675296982928 -> 1675296982880
	1675278293488 [label="blocks.5.6.bn2.weight
 (1536)" fillcolor=lightblue]
	1675278293488 -> 1675296982928
	1675296982928 [label=AccumulateGrad]
	1675296982976 -> 1675296982880
	1675278293648 [label="blocks.5.6.bn2.bias
 (1536)" fillcolor=lightblue]
	1675278293648 -> 1675296982976
	1675296982976 [label=AccumulateGrad]
	1675296983216 -> 1675296983408
	1675296983216 [label=SigmoidBackward0]
	1675296982640 -> 1675296983216
	1675296982640 [label=ConvolutionBackward0]
	1675296981920 -> 1675296982640
	1675296981920 [label=SiluBackward0]
	1675296981968 -> 1675296981920
	1675296981968 [label=ConvolutionBackward0]
	1675296981200 -> 1675296981968
	1675296981200 [label=MeanBackward1]
	1675296983120 -> 1675296981200
	1675296951696 -> 1675296981968
	1675278294048 [label="blocks.5.6.se.conv_reduce.weight
 (64, 1536, 1, 1)" fillcolor=lightblue]
	1675278294048 -> 1675296951696
	1675296951696 [label=AccumulateGrad]
	1675296951216 -> 1675296981968
	1675278294128 [label="blocks.5.6.se.conv_reduce.bias
 (64)" fillcolor=lightblue]
	1675278294128 -> 1675296951216
	1675296951216 [label=AccumulateGrad]
	1675296981872 -> 1675296982640
	1675278294288 [label="blocks.5.6.se.conv_expand.weight
 (1536, 64, 1, 1)" fillcolor=lightblue]
	1675278294288 -> 1675296981872
	1675296981872 [label=AccumulateGrad]
	1675296983024 -> 1675296982640
	1675278294368 [label="blocks.5.6.se.conv_expand.bias
 (1536)" fillcolor=lightblue]
	1675278294368 -> 1675296983024
	1675296983024 [label=AccumulateGrad]
	1675296983504 -> 1675296983792
	1675278294528 [label="blocks.5.6.conv_pwl.weight
 (256, 1536, 1, 1)" fillcolor=lightblue]
	1675278294528 -> 1675296983504
	1675296983504 [label=AccumulateGrad]
	1675296983888 -> 1675296983984
	1675278294608 [label="blocks.5.6.bn3.weight
 (256)" fillcolor=lightblue]
	1675278294608 -> 1675296983888
	1675296983888 [label=AccumulateGrad]
	1675296983840 -> 1675296983984
	1675278294688 [label="blocks.5.6.bn3.bias
 (256)" fillcolor=lightblue]
	1675278294688 -> 1675296983840
	1675296983840 [label=AccumulateGrad]
	1675296983936 -> 1675297015072
	1675296984080 -> 1675296984368
	1675278495968 [label="blocks.5.7.conv_pw.weight
 (1536, 256, 1, 1)" fillcolor=lightblue]
	1675278495968 -> 1675296984080
	1675296984080 [label=AccumulateGrad]
	1675296984464 -> 1675296984560
	1675278496048 [label="blocks.5.7.bn1.weight
 (1536)" fillcolor=lightblue]
	1675278496048 -> 1675296984464
	1675296984464 [label=AccumulateGrad]
	1675296984656 -> 1675296984560
	1675278496128 [label="blocks.5.7.bn1.bias
 (1536)" fillcolor=lightblue]
	1675278496128 -> 1675296984656
	1675296984656 [label=AccumulateGrad]
	1675296984848 -> 1675297013872
	1675278496608 [label="blocks.5.7.conv_dw.weight
 (1536, 1, 3, 3)" fillcolor=lightblue]
	1675278496608 -> 1675296984848
	1675296984848 [label=AccumulateGrad]
	1675297013968 -> 1675297014064
	1675278496528 [label="blocks.5.7.bn2.weight
 (1536)" fillcolor=lightblue]
	1675278496528 -> 1675297013968
	1675297013968 [label=AccumulateGrad]
	1675297014112 -> 1675297014064
	1675278496688 [label="blocks.5.7.bn2.bias
 (1536)" fillcolor=lightblue]
	1675278496688 -> 1675297014112
	1675297014112 [label=AccumulateGrad]
	1675297014208 -> 1675297014448
	1675297014208 [label=SigmoidBackward0]
	1675297014160 -> 1675297014208
	1675297014160 [label=ConvolutionBackward0]
	1675296984224 -> 1675297014160
	1675296984224 [label=SiluBackward0]
	1675296984272 -> 1675296984224
	1675296984272 [label=ConvolutionBackward0]
	1675296982736 -> 1675296984272
	1675296982736 [label=MeanBackward1]
	1675297014256 -> 1675296982736
	1675296982256 -> 1675296984272
	1675278497088 [label="blocks.5.7.se.conv_reduce.weight
 (64, 1536, 1, 1)" fillcolor=lightblue]
	1675278497088 -> 1675296982256
	1675296982256 [label=AccumulateGrad]
	1675296983600 -> 1675296984272
	1675278497168 [label="blocks.5.7.se.conv_reduce.bias
 (64)" fillcolor=lightblue]
	1675278497168 -> 1675296983600
	1675296983600 [label=AccumulateGrad]
	1675296984176 -> 1675297014160
	1675278497328 [label="blocks.5.7.se.conv_expand.weight
 (1536, 64, 1, 1)" fillcolor=lightblue]
	1675278497328 -> 1675296984176
	1675296984176 [label=AccumulateGrad]
	1675296985040 -> 1675297014160
	1675278497408 [label="blocks.5.7.se.conv_expand.bias
 (1536)" fillcolor=lightblue]
	1675278497408 -> 1675296985040
	1675296985040 [label=AccumulateGrad]
	1675297014544 -> 1675297014832
	1675278497568 [label="blocks.5.7.conv_pwl.weight
 (256, 1536, 1, 1)" fillcolor=lightblue]
	1675278497568 -> 1675297014544
	1675297014544 [label=AccumulateGrad]
	1675297014928 -> 1675297015120
	1675278497648 [label="blocks.5.7.bn3.weight
 (256)" fillcolor=lightblue]
	1675278497648 -> 1675297014928
	1675297014928 [label=AccumulateGrad]
	1675297015024 -> 1675297015120
	1675278497728 [label="blocks.5.7.bn3.bias
 (256)" fillcolor=lightblue]
	1675278497728 -> 1675297015024
	1675297015024 [label=AccumulateGrad]
	1675297015072 -> 1675297017520
	1675297015216 -> 1675297015504
	1675278498208 [label="blocks.5.8.conv_pw.weight
 (1536, 256, 1, 1)" fillcolor=lightblue]
	1675278498208 -> 1675297015216
	1675297015216 [label=AccumulateGrad]
	1675297015600 -> 1675297015696
	1675278498288 [label="blocks.5.8.bn1.weight
 (1536)" fillcolor=lightblue]
	1675278498288 -> 1675297015600
	1675297015600 [label=AccumulateGrad]
	1675297015888 -> 1675297015696
	1675278498368 [label="blocks.5.8.bn1.bias
 (1536)" fillcolor=lightblue]
	1675278498368 -> 1675297015888
	1675297015888 [label=AccumulateGrad]
	1675297016080 -> 1675297016128
	1675278498848 [label="blocks.5.8.conv_dw.weight
 (1536, 1, 3, 3)" fillcolor=lightblue]
	1675278498848 -> 1675297016080
	1675297016080 [label=AccumulateGrad]
	1675297016272 -> 1675297016368
	1675278498768 [label="blocks.5.8.bn2.weight
 (1536)" fillcolor=lightblue]
	1675278498768 -> 1675297016272
	1675297016272 [label=AccumulateGrad]
	1675297016416 -> 1675297016368
	1675278498928 [label="blocks.5.8.bn2.bias
 (1536)" fillcolor=lightblue]
	1675278498928 -> 1675297016416
	1675297016416 [label=AccumulateGrad]
	1675297016656 -> 1675297016704
	1675297016656 [label=SigmoidBackward0]
	1675297016032 -> 1675297016656
	1675297016032 [label=ConvolutionBackward0]
	1675297015408 -> 1675297016032
	1675297015408 [label=SiluBackward0]
	1675297015312 -> 1675297015408
	1675297015312 [label=ConvolutionBackward0]
	1675297014352 -> 1675297015312
	1675297014352 [label=MeanBackward1]
	1675297016560 -> 1675297014352
	1675297014640 -> 1675297015312
	1675278499328 [label="blocks.5.8.se.conv_reduce.weight
 (64, 1536, 1, 1)" fillcolor=lightblue]
	1675278499328 -> 1675297014640
	1675297014640 [label=AccumulateGrad]
	1675296984512 -> 1675297015312
	1675278499408 [label="blocks.5.8.se.conv_reduce.bias
 (64)" fillcolor=lightblue]
	1675278499408 -> 1675296984512
	1675296984512 [label=AccumulateGrad]
	1675297015168 -> 1675297016032
	1675278499568 [label="blocks.5.8.se.conv_expand.weight
 (1536, 64, 1, 1)" fillcolor=lightblue]
	1675278499568 -> 1675297015168
	1675297015168 [label=AccumulateGrad]
	1675297016464 -> 1675297016032
	1675278499648 [label="blocks.5.8.se.conv_expand.bias
 (1536)" fillcolor=lightblue]
	1675278499648 -> 1675297016464
	1675297016464 [label=AccumulateGrad]
	1675297016848 -> 1675297017136
	1675278676032 [label="blocks.5.8.conv_pwl.weight
 (256, 1536, 1, 1)" fillcolor=lightblue]
	1675278676032 -> 1675297016848
	1675297016848 [label=AccumulateGrad]
	1675297017232 -> 1675297017424
	1675278676112 [label="blocks.5.8.bn3.weight
 (256)" fillcolor=lightblue]
	1675278676112 -> 1675297017232
	1675297017232 [label=AccumulateGrad]
	1675297017328 -> 1675297017424
	1675278676192 [label="blocks.5.8.bn3.bias
 (256)" fillcolor=lightblue]
	1675278676192 -> 1675297017328
	1675297017328 [label=AccumulateGrad]
	1675297017520 -> 1675297048560
	1675297017616 -> 1675297046640
	1675278676672 [label="blocks.5.9.conv_pw.weight
 (1536, 256, 1, 1)" fillcolor=lightblue]
	1675278676672 -> 1675297017616
	1675297017616 [label=AccumulateGrad]
	1675297046928 -> 1675297046736
	1675278676752 [label="blocks.5.9.bn1.weight
 (1536)" fillcolor=lightblue]
	1675278676752 -> 1675297046928
	1675297046928 [label=AccumulateGrad]
	1675297017808 -> 1675297046736
	1675278676832 [label="blocks.5.9.bn1.bias
 (1536)" fillcolor=lightblue]
	1675278676832 -> 1675297017808
	1675297017808 [label=AccumulateGrad]
	1675297047120 -> 1675297047264
	1675278677312 [label="blocks.5.9.conv_dw.weight
 (1536, 1, 3, 3)" fillcolor=lightblue]
	1675278677312 -> 1675297047120
	1675297047120 [label=AccumulateGrad]
	1675297047408 -> 1675297047360
	1675278677232 [label="blocks.5.9.bn2.weight
 (1536)" fillcolor=lightblue]
	1675278677232 -> 1675297047408
	1675297047408 [label=AccumulateGrad]
	1675297047600 -> 1675297047360
	1675278677392 [label="blocks.5.9.bn2.bias
 (1536)" fillcolor=lightblue]
	1675278677392 -> 1675297047600
	1675297047600 [label=AccumulateGrad]
	1675297047792 -> 1675297047984
	1675297047792 [label=SigmoidBackward0]
	1675297047216 -> 1675297047792
	1675297047216 [label=ConvolutionBackward0]
	1675297046832 -> 1675297047216
	1675297046832 [label=SiluBackward0]
	1675297017712 -> 1675297046832
	1675297017712 [label=ConvolutionBackward0]
	1675297016176 -> 1675297017712
	1675297016176 [label=MeanBackward1]
	1675297047696 -> 1675297016176
	1675297015792 -> 1675297017712
	1675278677792 [label="blocks.5.9.se.conv_reduce.weight
 (64, 1536, 1, 1)" fillcolor=lightblue]
	1675278677792 -> 1675297015792
	1675297015792 [label=AccumulateGrad]
	1675297016944 -> 1675297017712
	1675278677872 [label="blocks.5.9.se.conv_reduce.bias
 (64)" fillcolor=lightblue]
	1675278677872 -> 1675297016944
	1675297016944 [label=AccumulateGrad]
	1675297047504 -> 1675297047216
	1675278678032 [label="blocks.5.9.se.conv_expand.weight
 (1536, 64, 1, 1)" fillcolor=lightblue]
	1675278678032 -> 1675297047504
	1675297047504 [label=AccumulateGrad]
	1675297017568 -> 1675297047216
	1675278678112 [label="blocks.5.9.se.conv_expand.bias
 (1536)" fillcolor=lightblue]
	1675278678112 -> 1675297017568
	1675297017568 [label=AccumulateGrad]
	1675297048080 -> 1675297048224
	1675278678272 [label="blocks.5.9.conv_pwl.weight
 (256, 1536, 1, 1)" fillcolor=lightblue]
	1675278678272 -> 1675297048080
	1675297048080 [label=AccumulateGrad]
	1675297048368 -> 1675297048464
	1675278678352 [label="blocks.5.9.bn3.weight
 (256)" fillcolor=lightblue]
	1675278678352 -> 1675297048368
	1675297048368 [label=AccumulateGrad]
	1675297048320 -> 1675297048464
	1675278678432 [label="blocks.5.9.bn3.bias
 (256)" fillcolor=lightblue]
	1675278678432 -> 1675297048320
	1675297048320 [label=AccumulateGrad]
	1675297048560 -> 1675297071360
	1675297048656 -> 1675297048944
	1675278678912 [label="blocks.5.10.conv_pw.weight
 (1536, 256, 1, 1)" fillcolor=lightblue]
	1675278678912 -> 1675297048656
	1675297048656 [label=AccumulateGrad]
	1675297048896 -> 1675297049040
	1675278678992 [label="blocks.5.10.bn1.weight
 (1536)" fillcolor=lightblue]
	1675278678992 -> 1675297048896
	1675297048896 [label=AccumulateGrad]
	1675297049232 -> 1675297049040
	1675278679072 [label="blocks.5.10.bn1.bias
 (1536)" fillcolor=lightblue]
	1675278679072 -> 1675297049232
	1675297049232 [label=AccumulateGrad]
	1675297049424 -> 1675297049712
	1675278679552 [label="blocks.5.10.conv_dw.weight
 (1536, 1, 3, 3)" fillcolor=lightblue]
	1675278679552 -> 1675297049424
	1675297049424 [label=AccumulateGrad]
	1675297049808 -> 1675297049760
	1675278679472 [label="blocks.5.10.bn2.weight
 (1536)" fillcolor=lightblue]
	1675278679472 -> 1675297049808
	1675297049808 [label=AccumulateGrad]
	1675297049856 -> 1675297049760
	1675278679632 [label="blocks.5.10.bn2.bias
 (1536)" fillcolor=lightblue]
	1675278679632 -> 1675297049856
	1675297049856 [label=AccumulateGrad]
	1675297050096 -> 1675297050288
	1675297050096 [label=SigmoidBackward0]
	1675297049520 -> 1675297050096
	1675297049520 [label=ConvolutionBackward0]
	1675297048848 -> 1675297049520
	1675297048848 [label=SiluBackward0]
	1675297048752 -> 1675297048848
	1675297048752 [label=ConvolutionBackward0]
	1675297047888 -> 1675297048752
	1675297047888 [label=MeanBackward1]
	1675297050000 -> 1675297047888
	1675297047312 -> 1675297048752
	1675278864448 [label="blocks.5.10.se.conv_reduce.weight
 (64, 1536, 1, 1)" fillcolor=lightblue]
	1675278864448 -> 1675297047312
	1675297047312 [label=AccumulateGrad]
	1675297048176 -> 1675297048752
	1675278864528 [label="blocks.5.10.se.conv_reduce.bias
 (64)" fillcolor=lightblue]
	1675278864528 -> 1675297048176
	1675297048176 [label=AccumulateGrad]
	1675297048608 -> 1675297049520
	1675278864688 [label="blocks.5.10.se.conv_expand.weight
 (1536, 64, 1, 1)" fillcolor=lightblue]
	1675278864688 -> 1675297048608
	1675297048608 [label=AccumulateGrad]
	1675297049904 -> 1675297049520
	1675278864768 [label="blocks.5.10.se.conv_expand.bias
 (1536)" fillcolor=lightblue]
	1675278864768 -> 1675297049904
	1675297049904 [label=AccumulateGrad]
	1675297050384 -> 1675297071216
	1675278864928 [label="blocks.5.10.conv_pwl.weight
 (256, 1536, 1, 1)" fillcolor=lightblue]
	1675278864928 -> 1675297050384
	1675297050384 [label=AccumulateGrad]
	1675297071312 -> 1675297071408
	1675278865008 [label="blocks.5.10.bn3.weight
 (256)" fillcolor=lightblue]
	1675278865008 -> 1675297071312
	1675297071312 [label=AccumulateGrad]
	1675297071264 -> 1675297071408
	1675278865088 [label="blocks.5.10.bn3.bias
 (256)" fillcolor=lightblue]
	1675278865088 -> 1675297071264
	1675297071264 [label=AccumulateGrad]
	1675297071360 -> 1675297073760
	1675297071504 -> 1675297071888
	1675278865568 [label="blocks.5.11.conv_pw.weight
 (1536, 256, 1, 1)" fillcolor=lightblue]
	1675278865568 -> 1675297071504
	1675297071504 [label=AccumulateGrad]
	1675297071984 -> 1675297072080
	1675278865648 [label="blocks.5.11.bn1.weight
 (1536)" fillcolor=lightblue]
	1675278865648 -> 1675297071984
	1675297071984 [label=AccumulateGrad]
	1675297072272 -> 1675297072080
	1675278865728 [label="blocks.5.11.bn1.bias
 (1536)" fillcolor=lightblue]
	1675278865728 -> 1675297072272
	1675297072272 [label=AccumulateGrad]
	1675297072368 -> 1675297072560
	1675278866208 [label="blocks.5.11.conv_dw.weight
 (1536, 1, 3, 3)" fillcolor=lightblue]
	1675278866208 -> 1675297072368
	1675297072368 [label=AccumulateGrad]
	1675297072656 -> 1675297072608
	1675278866128 [label="blocks.5.11.bn2.weight
 (1536)" fillcolor=lightblue]
	1675278866128 -> 1675297072656
	1675297072656 [label=AccumulateGrad]
	1675297072848 -> 1675297072608
	1675278866288 [label="blocks.5.11.bn2.bias
 (1536)" fillcolor=lightblue]
	1675278866288 -> 1675297072848
	1675297072848 [label=AccumulateGrad]
	1675297072896 -> 1675297073136
	1675297072896 [label=SigmoidBackward0]
	1675297072320 -> 1675297072896
	1675297072320 [label=ConvolutionBackward0]
	1675297071792 -> 1675297072320
	1675297071792 [label=SiluBackward0]
	1675297071696 -> 1675297071792
	1675297071696 [label=ConvolutionBackward0]
	1675297049616 -> 1675297071696
	1675297049616 [label=MeanBackward1]
	1675297072944 -> 1675297049616
	1675297049136 -> 1675297071696
	1675278866688 [label="blocks.5.11.se.conv_reduce.weight
 (64, 1536, 1, 1)" fillcolor=lightblue]
	1675278866688 -> 1675297049136
	1675297049136 [label=AccumulateGrad]
	1675297050480 -> 1675297071696
	1675278866768 [label="blocks.5.11.se.conv_reduce.bias
 (64)" fillcolor=lightblue]
	1675278866768 -> 1675297050480
	1675297050480 [label=AccumulateGrad]
	1675297071600 -> 1675297072320
	1675278866928 [label="blocks.5.11.se.conv_expand.weight
 (1536, 64, 1, 1)" fillcolor=lightblue]
	1675278866928 -> 1675297071600
	1675297071600 [label=AccumulateGrad]
	1675297072752 -> 1675297072320
	1675278867008 [label="blocks.5.11.se.conv_expand.bias
 (1536)" fillcolor=lightblue]
	1675278867008 -> 1675297072752
	1675297072752 [label=AccumulateGrad]
	1675297073232 -> 1675297073520
	1675278867168 [label="blocks.5.11.conv_pwl.weight
 (256, 1536, 1, 1)" fillcolor=lightblue]
	1675278867168 -> 1675297073232
	1675297073232 [label=AccumulateGrad]
	1675297073616 -> 1675297073808
	1675278867248 [label="blocks.5.11.bn3.weight
 (256)" fillcolor=lightblue]
	1675278867248 -> 1675297073616
	1675297073616 [label=AccumulateGrad]
	1675297073712 -> 1675297073808
	1675278867328 [label="blocks.5.11.bn3.bias
 (256)" fillcolor=lightblue]
	1675278867328 -> 1675297073712
	1675297073712 [label=AccumulateGrad]
	1675297073760 -> 1675297104800
	1675297073904 -> 1675297074192
	1675278867808 [label="blocks.5.12.conv_pw.weight
 (1536, 256, 1, 1)" fillcolor=lightblue]
	1675278867808 -> 1675297073904
	1675297073904 [label=AccumulateGrad]
	1675297074288 -> 1675297074384
	1675278867888 [label="blocks.5.12.bn1.weight
 (1536)" fillcolor=lightblue]
	1675278867888 -> 1675297074288
	1675297074288 [label=AccumulateGrad]
	1675297074576 -> 1675297074384
	1675278867968 [label="blocks.5.12.bn1.bias
 (1536)" fillcolor=lightblue]
	1675278867968 -> 1675297074576
	1675297074576 [label=AccumulateGrad]
	1675297074768 -> 1675297074816
	1675279048768 [label="blocks.5.12.conv_dw.weight
 (1536, 1, 3, 3)" fillcolor=lightblue]
	1675279048768 -> 1675297074768
	1675297074768 [label=AccumulateGrad]
	1675297074960 -> 1675297103984
	1675278868368 [label="blocks.5.12.bn2.weight
 (1536)" fillcolor=lightblue]
	1675278868368 -> 1675297074960
	1675297074960 [label=AccumulateGrad]
	1675297075152 -> 1675297103984
	1675279048848 [label="blocks.5.12.bn2.bias
 (1536)" fillcolor=lightblue]
	1675279048848 -> 1675297075152
	1675297075152 [label=AccumulateGrad]
	1675297104176 -> 1675297104368
	1675297104176 [label=SigmoidBackward0]
	1675297074720 -> 1675297104176
	1675297074720 [label=ConvolutionBackward0]
	1675297074096 -> 1675297074720
	1675297074096 [label=SiluBackward0]
	1675297074000 -> 1675297074096
	1675297074000 [label=ConvolutionBackward0]
	1675297072464 -> 1675297074000
	1675297072464 [label=MeanBackward1]
	1675297104080 -> 1675297072464
	1675297072176 -> 1675297074000
	1675279049248 [label="blocks.5.12.se.conv_reduce.weight
 (64, 1536, 1, 1)" fillcolor=lightblue]
	1675279049248 -> 1675297072176
	1675297072176 [label=AccumulateGrad]
	1675297073328 -> 1675297074000
	1675279049328 [label="blocks.5.12.se.conv_reduce.bias
 (64)" fillcolor=lightblue]
	1675279049328 -> 1675297073328
	1675297073328 [label=AccumulateGrad]
	1675297073856 -> 1675297074720
	1675279049488 [label="blocks.5.12.se.conv_expand.weight
 (1536, 64, 1, 1)" fillcolor=lightblue]
	1675279049488 -> 1675297073856
	1675297073856 [label=AccumulateGrad]
	1675297075056 -> 1675297074720
	1675279049568 [label="blocks.5.12.se.conv_expand.bias
 (1536)" fillcolor=lightblue]
	1675279049568 -> 1675297075056
	1675297075056 [label=AccumulateGrad]
	1675297104464 -> 1675297104512
	1675279049728 [label="blocks.5.12.conv_pwl.weight
 (256, 1536, 1, 1)" fillcolor=lightblue]
	1675279049728 -> 1675297104464
	1675297104464 [label=AccumulateGrad]
	1675297104656 -> 1675297104848
	1675279049808 [label="blocks.5.12.bn3.weight
 (256)" fillcolor=lightblue]
	1675279049808 -> 1675297104656
	1675297104656 [label=AccumulateGrad]
	1675297104752 -> 1675297104848
	1675279049888 [label="blocks.5.12.bn3.bias
 (256)" fillcolor=lightblue]
	1675279049888 -> 1675297104752
	1675297104752 [label=AccumulateGrad]
	1675297104800 -> 1675297107248
	1675297104944 -> 1675297105232
	1675279050368 [label="blocks.5.13.conv_pw.weight
 (1536, 256, 1, 1)" fillcolor=lightblue]
	1675279050368 -> 1675297104944
	1675297104944 [label=AccumulateGrad]
	1675297105328 -> 1675297105424
	1675279050448 [label="blocks.5.13.bn1.weight
 (1536)" fillcolor=lightblue]
	1675279050448 -> 1675297105328
	1675297105328 [label=AccumulateGrad]
	1675297105616 -> 1675297105424
	1675279050528 [label="blocks.5.13.bn1.bias
 (1536)" fillcolor=lightblue]
	1675279050528 -> 1675297105616
	1675297105616 [label=AccumulateGrad]
	1675297105808 -> 1675297105952
	1675279051008 [label="blocks.5.13.conv_dw.weight
 (1536, 1, 3, 3)" fillcolor=lightblue]
	1675279051008 -> 1675297105808
	1675297105808 [label=AccumulateGrad]
	1675297106096 -> 1675297106048
	1675279050928 [label="blocks.5.13.bn2.weight
 (1536)" fillcolor=lightblue]
	1675279050928 -> 1675297106096
	1675297106096 [label=AccumulateGrad]
	1675297106288 -> 1675297106048
	1675279051088 [label="blocks.5.13.bn2.bias
 (1536)" fillcolor=lightblue]
	1675279051088 -> 1675297106288
	1675297106288 [label=AccumulateGrad]
	1675297106480 -> 1675297106672
	1675297106480 [label=SigmoidBackward0]
	1675297105904 -> 1675297106480
	1675297105904 [label=ConvolutionBackward0]
	1675297105088 -> 1675297105904
	1675297105088 [label=SiluBackward0]
	1675297105136 -> 1675297105088
	1675297105136 [label=ConvolutionBackward0]
	1675297104272 -> 1675297105136
	1675297104272 [label=MeanBackward1]
	1675297106384 -> 1675297104272
	1675297104416 -> 1675297105136
	1675279051488 [label="blocks.5.13.se.conv_reduce.weight
 (64, 1536, 1, 1)" fillcolor=lightblue]
	1675279051488 -> 1675297104416
	1675297104416 [label=AccumulateGrad]
	1675297074480 -> 1675297105136
	1675279051568 [label="blocks.5.13.se.conv_reduce.bias
 (64)" fillcolor=lightblue]
	1675279051568 -> 1675297074480
	1675297074480 [label=AccumulateGrad]
	1675297105040 -> 1675297105904
	1675279051728 [label="blocks.5.13.se.conv_expand.weight
 (1536, 64, 1, 1)" fillcolor=lightblue]
	1675279051728 -> 1675297105040
	1675297105040 [label=AccumulateGrad]
	1675297106192 -> 1675297105904
	1675279051808 [label="blocks.5.13.se.conv_expand.bias
 (1536)" fillcolor=lightblue]
	1675279051808 -> 1675297106192
	1675297106192 [label=AccumulateGrad]
	1675297106768 -> 1675297106912
	1675279051968 [label="blocks.5.13.conv_pwl.weight
 (256, 1536, 1, 1)" fillcolor=lightblue]
	1675279051968 -> 1675297106768
	1675297106768 [label=AccumulateGrad]
	1675297107056 -> 1675297107152
	1675279052048 [label="blocks.5.13.bn3.weight
 (256)" fillcolor=lightblue]
	1675279052048 -> 1675297107056
	1675297107056 [label=AccumulateGrad]
	1675297107008 -> 1675297107152
	1675279052128 [label="blocks.5.13.bn3.bias
 (256)" fillcolor=lightblue]
	1675279052128 -> 1675297107008
	1675297107008 [label=AccumulateGrad]
	1675297107248 -> 1675297134144
	1675297107344 -> 1675297107728
	1675279052608 [label="blocks.5.14.conv_pw.weight
 (1536, 256, 1, 1)" fillcolor=lightblue]
	1675279052608 -> 1675297107344
	1675297107344 [label=AccumulateGrad]
	1675297107824 -> 1675297132656
	1675279052688 [label="blocks.5.14.bn1.weight
 (1536)" fillcolor=lightblue]
	1675279052688 -> 1675297107824
	1675297107824 [label=AccumulateGrad]
	1675297107920 -> 1675297132656
	1675279241280 [label="blocks.5.14.bn1.bias
 (1536)" fillcolor=lightblue]
	1675279241280 -> 1675297107920
	1675297107920 [label=AccumulateGrad]
	1675297132704 -> 1675297132896
	1675279241760 [label="blocks.5.14.conv_dw.weight
 (1536, 1, 3, 3)" fillcolor=lightblue]
	1675279241760 -> 1675297132704
	1675297132704 [label=AccumulateGrad]
	1675297133040 -> 1675297133136
	1675279241680 [label="blocks.5.14.bn2.weight
 (1536)" fillcolor=lightblue]
	1675279241680 -> 1675297133040
	1675297133040 [label=AccumulateGrad]
	1675297133184 -> 1675297133136
	1675279241840 [label="blocks.5.14.bn2.bias
 (1536)" fillcolor=lightblue]
	1675279241840 -> 1675297133184
	1675297133184 [label=AccumulateGrad]
	1675297133424 -> 1675297133616
	1675297133424 [label=SigmoidBackward0]
	1675297132848 -> 1675297133424
	1675297132848 [label=ConvolutionBackward0]
	1675297132608 -> 1675297132848
	1675297132608 [label=SiluBackward0]
	1675297107536 -> 1675297132608
	1675297107536 [label=ConvolutionBackward0]
	1675297106000 -> 1675297107536
	1675297106000 [label=MeanBackward1]
	1675297133328 -> 1675297106000
	1675297105520 -> 1675297107536
	1675279242240 [label="blocks.5.14.se.conv_reduce.weight
 (64, 1536, 1, 1)" fillcolor=lightblue]
	1675279242240 -> 1675297105520
	1675297105520 [label=AccumulateGrad]
	1675297106864 -> 1675297107536
	1675279242320 [label="blocks.5.14.se.conv_reduce.bias
 (64)" fillcolor=lightblue]
	1675279242320 -> 1675297106864
	1675297106864 [label=AccumulateGrad]
	1675297133232 -> 1675297132848
	1675279242480 [label="blocks.5.14.se.conv_expand.weight
 (1536, 64, 1, 1)" fillcolor=lightblue]
	1675279242480 -> 1675297133232
	1675297133232 [label=AccumulateGrad]
	1675297107440 -> 1675297132848
	1675279242560 [label="blocks.5.14.se.conv_expand.bias
 (1536)" fillcolor=lightblue]
	1675279242560 -> 1675297107440
	1675297107440 [label=AccumulateGrad]
	1675297133712 -> 1675297134000
	1675279242720 [label="blocks.5.14.conv_pwl.weight
 (256, 1536, 1, 1)" fillcolor=lightblue]
	1675279242720 -> 1675297133712
	1675297133712 [label=AccumulateGrad]
	1675297134096 -> 1675297134192
	1675279242800 [label="blocks.5.14.bn3.weight
 (256)" fillcolor=lightblue]
	1675279242800 -> 1675297134096
	1675297134096 [label=AccumulateGrad]
	1675297134048 -> 1675297134192
	1675279242880 [label="blocks.5.14.bn3.bias
 (256)" fillcolor=lightblue]
	1675279242880 -> 1675297134048
	1675297134048 [label=AccumulateGrad]
	1675297134144 -> 1675297134384
	1675297134480 -> 1675297134768
	1675279243120 [label="conv_head.weight
 (1280, 256, 1, 1)" fillcolor=lightblue]
	1675279243120 -> 1675297134480
	1675297134480 [label=AccumulateGrad]
	1675297134864 -> 1675297134960
	1675279243200 [label="bn2.weight
 (1280)" fillcolor=lightblue]
	1675279243200 -> 1675297134864
	1675297134864 [label=AccumulateGrad]
	1675297135104 -> 1675297134960
	1675279243280 [label="bn2.bias
 (1280)" fillcolor=lightblue]
	1675279243280 -> 1675297135104
	1675297135104 [label=AccumulateGrad]
	1675297135632 -> 1675297135440
	1675297135632 [label=TBackward0]
	1675297135056 -> 1675297135632
	1675193232816 [label="classifier.weight
 (10, 1280)" fillcolor=lightblue]
	1675193232816 -> 1675297135056
	1675297135056 [label=AccumulateGrad]
	1675297135536 -> 1675296937920
}
